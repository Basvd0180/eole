"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[8139],{6819:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var s=t(4848),i=t(8453);const o={},r="What special tokens are used?",a={id:"FAQ/special_tokens",title:"What special tokens are used?",description:"There are 4 main special tokens:",source:"@site/docs/FAQ/special_tokens.md",sourceDirName:"FAQ",slug:"/FAQ/special_tokens",permalink:"/eole/docs/FAQ/special_tokens",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/special_tokens.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How do I use Pretrained embeddings (e.g. GloVe)?",permalink:"/eole/docs/FAQ/pretrained_embeddings"},next:{title:"How can I apply on-the-fly tokenization and subword regularization when training?",permalink:"/eole/docs/FAQ/tokenization"}},l={},c=[{value:"Special tokens actually used",id:"special-tokens-actually-used",level:2},{value:"Special tokens behaviour in Eole",id:"special-tokens-behaviour-in-eole",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"what-special-tokens-are-used",children:"What special tokens are used?"}),"\n",(0,s.jsx)(n.p,{children:"There are 4 main special tokens:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'BOS for "beginning of sentence";'}),"\n",(0,s.jsx)(n.li,{children:'PAD for "padding";'}),"\n",(0,s.jsx)(n.li,{children:'EOS for "end of sentence";'}),"\n",(0,s.jsx)(n.li,{children:'UNK for "unknown".'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"special-tokens-actually-used",children:"Special tokens actually used"}),"\n",(0,s.jsx)(n.p,{children:"Depending on the context, these tokens can take various values:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Default behaviour, training from scratch"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Some default values are defined as ",(0,s.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/ff39275c50d12951963008da11d029940b590713/eole/constants.py#L8",children:"constants"})," for the project:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class DefaultTokens(object):\n    PAD = "<blank>"\n    BOS = "<s>"\n    EOS = "</s>"\n    UNK = "<unk>"\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Retrieving a pretrained model from HF"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The special tokens will be retrieved and configured from the ",(0,s.jsx)(n.code,{children:"special_tokens_map.json"})," configuration file from the HF model files."]}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Custom behaviour"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In any case, these tokens can be overriden via the ad-hoc configuration settings:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"bos_token"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"pad_token"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"eos_token"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"unk_token"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"special-tokens-behaviour-in-eole",children:"Special tokens behaviour in Eole"}),"\n",(0,s.jsx)(n.p,{children:"When we train a SEQ2SEQ model we use:\nSRC: srctok1 srctok2 srctok3 .... srctokn\nTGT: BOS tgttok1 tgttok2 ..... tgttokm EOS\nBut when training a LM\nSRC: BOS srctok1 srctok2 srctok3 .... srctokn\nTGT: srctok1 srctok2 srctok3 .... srctokn EOS"}),"\n",(0,s.jsx)(n.p,{children:"Having said that, sometimes we need to finetune models (eg: NLLB-200, Llama, ...) with existing vocab\nand special tokens are not the same."}),"\n",(0,s.jsx)(n.p,{children:"ex with NLLB-200\nBOS id=0\nPAD id=1\nEOS id=2\nUNK id=3\nAnd the decoder start token is EOS () which means in fact that the BOS is never used.\nAt training, TGT needs to start with EOS instead of BOS in the default OpenNMT-py config."}),"\n",(0,s.jsx)(n.p,{children:"Example of Llama\nUNK id=0\nBOS id=1\nEOS id=2\nThere was no PAD but to avoid conflicts we forced PAD id=3 (which was token '<0x00>' in the original llama tokenizer)"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);