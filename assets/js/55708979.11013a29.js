"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[6021],{7214:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var n=i(4848),a=i(8453);const s={},r="Language Model Wiki-103",o={id:"recipes/wiki_103/README",title:"Language Model Wiki-103",description:"Step 1: Download and clean the data, prepare subword model",source:"@site/docs/recipes/wiki_103/README.md",sourceDirName:"recipes/wiki_103",slug:"/recipes/wiki_103/",permalink:"/eole/docs/recipes/wiki_103/",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/recipes/wiki_103/README.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Serving models with Eole",permalink:"/eole/docs/recipes/server/"},next:{title:"Translation WMT17 en-de",permalink:"/eole/docs/recipes/wmt17/"}},l={},d=[{value:"Step 1: Download and clean the data, prepare subword model",id:"step-1-download-and-clean-the-data-prepare-subword-model",level:2},{value:"Step 2: Build the vocabulary",id:"step-2-build-the-vocabulary",level:2},{value:"Language Model specificities",id:"language-model-specificities",level:3},{value:"BPE specificities",id:"bpe-specificities",level:3},{value:"Build vocabulary command",id:"build-vocabulary-command",level:3},{value:"Step 3: Train the model",id:"step-3-train-the-model",level:2},{value:"Step 4: Generate output",id:"step-4-generate-output",level:2}];function c(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"language-model-wiki-103",children:"Language Model Wiki-103"}),"\n",(0,n.jsx)(t.h2,{id:"step-1-download-and-clean-the-data-prepare-subword-model",children:"Step 1: Download and clean the data, prepare subword model"}),"\n",(0,n.jsxs)(t.p,{children:["Preliminary steps are defined in the ",(0,n.jsx)(t.code,{children:"prepare_wikitext-103_data.sh"})," script."]}),"\n",(0,n.jsxs)(t.p,{children:["The following command will download the ",(0,n.jsx)(t.a,{href:"https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/",children:"WikiText103 dataset"}),", remove empty lines and shuffle the training corpus. It will also call the ",(0,n.jsx)(t.code,{children:"learn_bpe.py"})," script to train a bpe of 40000 symbols on the training dataset using pyonmttok. The bpe model will be stored in ",(0,n.jsx)(t.code,{children:"data/wikitext/wikitext-103-raw-v1/subwords.bpe"}),"."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"chmod u+x prepare_wikitext-103_data.sh\n./prepare_wikitext-103_data.sh\n"})}),"\n",(0,n.jsx)(t.h2,{id:"step-2-build-the-vocabulary",children:"Step 2: Build the vocabulary"}),"\n",(0,n.jsxs)(t.p,{children:["An example of yaml configuration for language modeling task is available in ",(0,n.jsx)(t.code,{children:"wiki_103.yaml"}),". This configuration will be used for building the vocabulary and training the model.\nBPE and language modeling specificities are explained in the following sections."]}),"\n",(0,n.jsx)(t.h3,{id:"language-model-specificities",children:"Language Model specificities"}),"\n",(0,n.jsx)(t.p,{children:"In LM tasks we expect a single source, therefore path_tgt is not required for LM tasks."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-yaml",children:"data:\n    corpus_1:\n        path_src: data/wikitext-103-raw/wiki.train.raw\n"})}),"\n",(0,n.jsx)(t.h3,{id:"bpe-specificities",children:"BPE specificities"}),"\n",(0,n.jsx)(t.p,{children:"To use BPE tokenization on the fly, the following parameters must be in the config file.\nSlight differences between on the fly tokenization and outputed tokenized files from step 1 can be observed."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-yaml",children:'transforms: [onmt_tokenize, filtertoolong]\ntransforms_configs:\n  onmt_tokenize:\n    src_subword_type: bpe\n    src_subword_model: data/wikitext-103-raw/subwords.bpe\n    src_onmttok_kwargs: {"mode": "aggressive", "joiner_annotate": True, "preserve_placeholders":\n    True, "case_markup": True, "soft_case_regions": True, "preserve_segmented_tokens":\n    True}\n'})}),"\n",(0,n.jsx)(t.h3,{id:"build-vocabulary-command",children:"Build vocabulary command"}),"\n",(0,n.jsx)(t.p,{children:"The vocabulary is built using:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"eole build_vocab -config wiki_103.yaml -n_sample -1 # -num_threads 4\n"})}),"\n",(0,n.jsx)(t.h2,{id:"step-3-train-the-model",children:"Step 3: Train the model"}),"\n",(0,n.jsx)(t.p,{children:"The training is launched using:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"eole train -config wiki_103.yaml\n"})}),"\n",(0,n.jsx)(t.p,{children:"Tensorboard can be used to monitor the training."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Expected results:"})," perplexity of 20-22 on the validation set."]}),"\n",(0,n.jsx)(t.h2,{id:"step-4-generate-output",children:"Step 4: Generate output"}),"\n",(0,n.jsx)(t.p,{children:"Options contained in the loaded model will trigger language modeling specific inference."}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.code,{children:"input.txt"})," must contain already tokenized examples, with the same method as the training data. Here, part of validation data will be used:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:'head data/wikitext-103-raw/wiki.valid.bpe | cut -d" " -f-15 > data/wikitext/wikitext-103-raw-v1/test_input.txt\n'})}),"\n",(0,n.jsxs)(t.p,{children:["To proceed with LM inference, sampling methods such as top-k sampling or nucleus sampling are usually applied. Details and options about inference methods can be found in ",(0,n.jsx)(t.a,{href:"https://github.com/eole-nlp/eole/tree/master/eole/config/inference.py",children:(0,n.jsx)(t.code,{children:"eole/config/inference.py"})}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["The following command will provide inference with nucleus sampling of p=0.9 and return the 3 sequences with the lowest perplexity out of the 10 generated sequences (see ",(0,n.jsx)(t.code,{children:"inference.yaml"}),"):"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"eole predict -config inference.yaml -model_path data/wikitext/wikitext-103-raw-v1/run/model-lm/step_1000000 -src data/wikitext/wikitext-103-raw-v1/test_input.txt -output data/wikitext/wikitext-103-raw-v1/test_pred.txt\n"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Note"}),": main transform-related settings are now stored within the model and its configuration, which makes the (rather complex) ",(0,n.jsx)(t.code,{children:"inference.yaml"})," config not strictly necessary anymore. The above command can be converted to a simple command line with the desired settings:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"eole predict -model_path data/wikitext/wikitext-103-raw-v1/run/model-lm/step_1000000 -src data/wikitext/wikitext-103-raw-v1/test_input.txt -output data/wikitext/wikitext-103-raw-v1/test_pred.txt -world_size 1 -gpu_ranks 0 -n_best 3 -top_p 0.9 -beam_size 10\n"})})]})}function p(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>r,x:()=>o});var n=i(6540);const a={},s=n.createContext(a);function r(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);