"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[3822],{1685:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var a=t(4848),o=t(8453);const r={sidebar_position:2},s="Quickstart",i={id:"quickstart",title:"Quickstart",description:"How to train a model from scratch",source:"@site/docs/quickstart.md",sourceDirName:".",slug:"/quickstart",permalink:"/eole/docs/quickstart",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/quickstart.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"EOLE",permalink:"/eole/docs/"},next:{title:"Key Concepts",permalink:"/eole/docs/category/key-concepts"}},l={},c=[{value:"How to train a model from scratch",id:"how-to-train-a-model-from-scratch",level:2},{value:"Step 1: Prepare the data",id:"step-1-prepare-the-data",level:3},{value:"Step 2: Train the model",id:"step-2-train-the-model",level:3},{value:"Step 3: Translate",id:"step-3-translate",level:3},{value:"How to generate with a pretrained LLM",id:"how-to-generate-with-a-pretrained-llm",level:2},{value:"Step 1: Convert a model from Hugging Face Hub",id:"step-1-convert-a-model-from-hugging-face-hub",level:3},{value:"Step 2: Prepare an inference.yaml config file",id:"step-2-prepare-an-inferenceyaml-config-file",level:3},{value:"Step 3: Generate text",id:"step-3-generate-text",level:3},{value:"How to finetune a pretrained LLM",id:"how-to-finetune-a-pretrained-llm",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"quickstart",children:"Quickstart"}),"\n",(0,a.jsx)(n.h2,{id:"how-to-train-a-model-from-scratch",children:"How to train a model from scratch"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-prepare-the-data",children:"Step 1: Prepare the data"}),"\n",(0,a.jsx)(n.p,{children:"To get started, we propose to download a toy English-German dataset for machine translation containing 10k tokenized sentences:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\ntar xf toy-ende.tar.gz\ncd toy-ende\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The data consists of parallel source (",(0,a.jsx)(n.code,{children:"src"}),") and target (",(0,a.jsx)(n.code,{children:"tgt"}),") data containing one sentence per line with tokens separated by a space:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"src-train.txt"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"tgt-train.txt"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"src-val.txt"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"tgt-val.txt"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Validation files are used to evaluate the convergence of the training. It usually contains no more than 5k sentences."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"$ head -n 2 toy-ende/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament &apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .\n"})}),"\n",(0,a.jsxs)(n.p,{children:["We need to build a ",(0,a.jsx)(n.strong,{children:"YAML configuration file"})," to specify the data that will be used:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# toy_en_de.yaml\n\n## Where the samples will be written\nsave_data: toy-ende/run/example\n## Where the vocab(s) will be written\nsrc_vocab: toy-ende/run/example.vocab.src\ntgt_vocab: toy-ende/run/example.vocab.tgt\n# Prevent overwriting existing files in the folder\noverwrite: False\n\n# Corpus opts:\ndata:\n    corpus_1:\n        path_src: toy-ende/src-train.txt\n        path_tgt: toy-ende/tgt-train.txt\n    valid:\n        path_src: toy-ende/src-val.txt\n        path_tgt: toy-ende/tgt-val.txt\n"})}),"\n",(0,a.jsx)(n.p,{children:"From this configuration, we can build the vocab(s), that will be necessary to train the model:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"eole build_vocab -config toy_en_de.yaml -n_sample 10000\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Notes"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-n_sample"})," is advised here \u2013 it represents the number of lines sampled from each corpus to build the vocab."]}),"\n",(0,a.jsxs)(n.li,{children:["This configuration is the simplest possible, without any tokenization or other ",(0,a.jsx)(n.em,{children:"transforms"}),". See ",(0,a.jsx)(n.a,{href:"/docs/category/recipes",children:"recipes"})," for more complex pipelines."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"step-2-train-the-model",children:"Step 2: Train the model"}),"\n",(0,a.jsxs)(n.p,{children:["To train a model, we need to ",(0,a.jsx)(n.strong,{children:"add the following to the YAML configuration file"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"the vocabulary path(s) that will be used: can be that generated by eole build_vocab;"}),"\n",(0,a.jsx)(n.li,{children:"training specific parameters."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"# toy_en_de.yaml\n\n# Model architecture\nmodel:\n  architecture: transformer\n\n# Train on a single GPU\ntraining:\n  world_size: 1\n  gpu_ranks: [0]\n  model_path: toy-ende/run/model\n  save_checkpoint_steps: 500\n  train_steps: 1000\n  valid_steps: 500\n  # adapt dataloading defaults to very small dataset\n  bucket_size: 1000\n"})}),"\n",(0,a.jsx)(n.p,{children:"Then you can simply run:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"eole train -config toy_en_de.yaml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This configuration will run a default transformer model. It will run on a single GPU (",(0,a.jsx)(n.code,{children:"world_size 1"})," & ",(0,a.jsx)(n.code,{children:"gpu_ranks [0]"}),")."]}),"\n",(0,a.jsxs)(n.p,{children:["Before the training process actually starts, it is possible to generate transformed samples to simplify any potentially required visual inspection. The number of sample lines to dump per corpus is set with the ",(0,a.jsx)(n.code,{children:"-n_sample"})," flag."]}),"\n",(0,a.jsx)(n.h3,{id:"step-3-translate",children:"Step 3: Translate"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"eole predict -model_path toy-ende/run/model -src toy-ende/src-test.txt -output toy-ende/pred_1000.txt -gpu 0 -verbose\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into ",(0,a.jsx)(n.code,{children:"toy-ende/pred_1000.txt"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note"}),":"]}),"\n",(0,a.jsx)(n.p,{children:"The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets!"}),"\n",(0,a.jsxs)(n.p,{children:["For example you can download millions of parallel sentences for ",(0,a.jsx)(n.a,{href:"http://www.statmt.org/wmt16/translation-task.html",children:"translation"})," or ",(0,a.jsx)(n.a,{href:"https://github.com/harvardnlp/sent-summary",children:"summarization"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"how-to-generate-with-a-pretrained-llm",children:"How to generate with a pretrained LLM"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-convert-a-model-from-hugging-face-hub",children:"Step 1: Convert a model from Hugging Face Hub"}),"\n",(0,a.jsx)(n.p,{children:"Several converters are provided for models 1) from Hugging Face hub: T5, Falcon, MPT, Openllama, Redpajama, Xgen or 2) the legacy Llama from Meta."}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"/docs/concepts/command_line#model-conversion-tools",children:"here"})," for conversion command line."]}),"\n",(0,a.jsx)(n.p,{children:"T5 (and variant Flan-T5), Llama and Openllama use Sentencepiece.\nOther models uses BPE, we had to reconstruct the BPE model and vocab file:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://opennmt-models.s3.amazonaws.com/mosaic-MPT/mpt-model.bpe",children:"MPT bpe model"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://opennmt-models.s3.amazonaws.com/mosaic-MPT/mpt.vocab",children:"MPT vocab"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://opennmt-models.s3.amazonaws.com/redpajama/redpajama-model.bpe",children:"Redpajama bpe model"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://opennmt-models.s3.amazonaws.com/redpajama/redpajama.vocab",children:"Redpajama vocab"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://opennmt-models.s3.amazonaws.com/falcon/falcon-model.bpe",children:"Falcon bpe model"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://opennmt-models.s3.amazonaws.com/falcon/falcon.vocab",children:"Falcon vocab"})}),"\n",(0,a.jsx)(n.p,{children:"The command line to convert a model to OpenNMT-py is:"}),"\n",(0,a.jsx)(n.p,{children:"Note: providing a HuggingFace repo id is supported in most conversion tools."}),"\n",(0,a.jsx)(n.h3,{id:"step-2-prepare-an-inferenceyaml-config-file",children:"Step 2: Prepare an inference.yaml config file"}),"\n",(0,a.jsx)(n.p,{children:"Even though it is not mandatory, the best way to run inference is to use a config file; here is an example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'transforms: [sentencepiece]\n\n#### Subword\nsrc_subword_model: "/path_to/llama7B/tokenizer.model"\ntgt_subword_model: "/path_to/llama7B/tokenizer.model"\n\n# Model info\nmodel_path: "/path_to/llama7B/llama7B-eole.pt"\n\n# Inference\nseed: 42\nmax_length: 256\ngpu: 0\nbatch_type: sents\nbatch_size: 1\nprecision: fp16\n#random_sampling_topk: 40\n#random_sampling_topp: 0.75\n#random_sampling_temp: 0.1\nbeam_size: 1\nn_best: 1\nreport_time: true\n'})}),"\n",(0,a.jsx)(n.p,{children:"or similarly for a model using BPE:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'transforms: [onmt_tokenize]\n\n#### Subword\nsrc_subword_type: bpe\nsrc_subword_model: "/path_to/mpt7B/mpt-model.bpe"\nsrc_onmttok_kwargs: \'{"mode": "conservative"}\'\n\ntgt_subword_type: bpe\ntgt_subword_model: "/path_to/mpt7B/mpt-model.bpe"\ntgt_onmttok_kwargs: \'{"mode": "conservative"}\'\ngpt2_pretok: true\n# Model info\nmodel_path: "/path_to/mpt7B/mpt-eole.pt"\n\n# Inference\nseed: 42\nmax_length: 1\ngpu: 0\nbatch_type: sents\nbatch_size: 1\nprecision: fp16\n#random_sampling_topk: 40\n#random_sampling_topp: 0.75\n#random_sampling_temp: 0.8\nbeam_size: 1\nreport_time: true\nsrc: None\ntgt: None\n'})}),"\n",(0,a.jsxs)(n.p,{children:["In this second example, we used ",(0,a.jsx)(n.code,{children:"max_length: 1"})," and ",(0,a.jsx)(n.code,{children:"src: None"})," ",(0,a.jsx)(n.code,{children:"tgt: None"})," which is typically the configuration to be used in a scoring script like MMLU where it expects only 1 token as the answer."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"WARNING"}),"\nFor inhomogeneous batches with many examples, the potentially high number of tokens inserted in the shortest examples leads to degraded results when attention layer quantization and flash attention are activated.\nIn practice, in the inference configuration file, when ",(0,a.jsx)(n.code,{children:"batch_size"})," is greater than 1,\ndelete \u2018linear_values\u2019, \u2018linear_query\u2019, \u2018linear_keys\u2019, \u2018final_linear\u2019 from ",(0,a.jsx)(n.code,{children:"quant_layers"})," and specify ",(0,a.jsx)(n.code,{children:"self_attn_type: scaled-dot"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"You can run this script with the following command line:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-default",children:"eole tools run_mmlu --config myinference.yaml\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-generate-text",children:"Step 3: Generate text"}),"\n",(0,a.jsx)(n.p,{children:"Generating text is also easier with an inference config file (in which you can set max_length or ramdom sampling settings):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-default",children:"eole predict --config /path_to_config/llama7B/llama-inference.yaml --src /path_to_source/input.txt --output /path_to_target/output.txt\n"})}),"\n",(0,a.jsx)(n.h2,{id:"how-to-finetune-a-pretrained-llm",children:"How to finetune a pretrained LLM"}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"/docs/recipes/llama2/",children:"Llama2 recipe"})," for an end-to-end example."]}),"\n",(0,a.jsx)(n.p,{children:"Note:"}),"\n",(0,a.jsxs)(n.p,{children:["If you want to enable the \u201czero-out prompt loss\u201d mechanism to ignore the prompt when calculating the loss,\nyou can add the ",(0,a.jsx)(n.code,{children:"insert_mask_before_placeholder"})," transform as well as the ",(0,a.jsx)(n.code,{children:"zero_out_prompt_loss"})," flag:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-default",children:"transforms: [insert_mask_before_placeholder, sentencepiece, filtertoolong]\nzero_out_prompt_loss: true\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The default value for the response ",(0,a.jsx)(n.code,{children:"response_pattern"})," used to locate the end of the prompt is \u201cResponse : \uff5fnewline\uff60\u201d, but you can choose another to align it with your training data."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>i});var a=t(6540);const o={},r=a.createContext(o);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);