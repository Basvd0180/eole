"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[4132],{2618:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var o=t(4848),s=t(8453);const a={},i="How can I apply on-the-fly tokenization and subword regularization when training?",r={id:"FAQ/tokenization",title:"How can I apply on-the-fly tokenization and subword regularization when training?",description:"This is part of the transforms paradigm, which allows to apply various processing to inputs before constituting batches to train models on (or predict). transforms basically is a list of functions that will be applied sequentially to the examples when read from file (or input list).",source:"@site/docs/FAQ/tokenization.md",sourceDirName:"FAQ",slug:"/FAQ/tokenization",permalink:"/eole/docs/FAQ/tokenization",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/tokenization.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"What special tokens are used?",permalink:"/eole/docs/FAQ/special_tokens"},next:{title:"How can I update a checkpoint's vocabulary?",permalink:"/eole/docs/FAQ/update_vocab"}},c={},l=[{value:"Example",id:"example",level:3}];function d(e){const n={code:"code",h1:"h1",h3:"h3",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"how-can-i-apply-on-the-fly-tokenization-and-subword-regularization-when-training",children:"How can I apply on-the-fly tokenization and subword regularization when training?"}),"\n",(0,o.jsxs)(n.p,{children:["This is part of the ",(0,o.jsx)(n.code,{children:"transforms"})," paradigm, which allows to apply various processing to inputs before constituting batches to train models on (or predict). ",(0,o.jsx)(n.code,{children:"transforms"})," basically is a ",(0,o.jsx)(n.code,{children:"list"})," of functions that will be applied sequentially to the examples when read from file (or input list).\nEach entry of the ",(0,o.jsx)(n.code,{children:"data"})," configuration can have its own ",(0,o.jsx)(n.code,{children:"transforms"}),", else, some default top-level ",(0,o.jsx)(n.code,{children:"transforms"})," can be set."]}),"\n",(0,o.jsxs)(n.p,{children:["For now, ",(0,o.jsx)(n.code,{children:"transforms"})," can only be configured at the general level (apart from a few exceptions like prefix/suffix), in the ",(0,o.jsx)(n.code,{children:"transforms_configs"})," section."]}),"\n",(0,o.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,o.jsxs)(n.p,{children:["This example applies sentencepiece tokenization with ",(0,o.jsx)(n.code,{children:"pyonmttok"}),", with ",(0,o.jsx)(n.code,{children:"nbest=20"})," and ",(0,o.jsx)(n.code,{children:"alpha=0.1"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# <your_config>.yaml\n\n...\ntransforms_configs:\n    onmt_tokenize:\n        # Tokenization options\n        src_subword_type: sentencepiece\n        src_subword_model: examples/subword.spm.model\n        tgt_subword_type: sentencepiece\n        tgt_subword_model: examples/subword.spm.model\n\n        # Number of candidates for SentencePiece sampling\n        subword_nbest: 20\n        # Smoothing parameter for SentencePiece sampling\n        subword_alpha: 0.1\n        # Specific arguments for pyonmttok\n        src_onmttok_kwargs: \"{'mode': 'none', 'spacer_annotate': True}\"\n        tgt_onmttok_kwargs: \"{'mode': 'none', 'spacer_annotate': True}\"\n\n# transforms: [onmt_tokenize] # if you don't need to specify at dataset level\n\n# Corpus opts:\ndata:\n    corpus_1:\n        path_src: toy-ende/src-train1.txt\n        path_tgt: toy-ende/tgt-train1.txt\n        transforms: [onmt_tokenize]\n        weight: 1\n    valid:\n        path_src: toy-ende/src-val.txt\n        path_tgt: toy-ende/tgt-val.txt\n        transforms: [onmt_tokenize]\n...\n\n"})}),"\n",(0,o.jsx)(n.p,{children:"Other tokenization methods and transforms are readily available. See the dedicated docs for more details."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);