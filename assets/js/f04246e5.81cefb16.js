"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[8139],{6819:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var s=n(4848),o=n(8453);const a={},i="What special tokens are used?",r={id:"FAQ/special_tokens",title:"What special tokens are used?",description:"In the v2, special tokens were different for SEQ2SEQ and LM:",source:"@site/docs/FAQ/special_tokens.md",sourceDirName:"FAQ",slug:"/FAQ/special_tokens",permalink:"/eole/docs/FAQ/special_tokens",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/special_tokens.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How do I use Pretrained embeddings (e.g. GloVe)?",permalink:"/eole/docs/FAQ/pretrained_embeddings"},next:{title:"How can I apply on-the-fly tokenization and subword regularization when training?",permalink:"/eole/docs/FAQ/tokenization"}},c={},d=[];function l(e){const t={h1:"h1",p:"p",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"what-special-tokens-are-used",children:"What special tokens are used?"}),"\n",(0,s.jsx)(t.p,{children:"In the v2, special tokens were different for SEQ2SEQ and LM:\nLM was BOS, PAD, EOS with IDs (0, 1, 2) and the first vocab token started at id=3\nSEQ2SEQ was UNK, PAD, BOS, EOS with IDs (0, 1, 2, 3) and first vocab token started at id=4"}),"\n",(0,s.jsx)(t.p,{children:'In v3 we changed this behavior to align things:\ngroup.add(\n"--default_specials",\n"-default_specilas",\nnargs="+",\ntype=str,\ndefault=[\nDefaultTokens.UNK,\nDefaultTokens.PAD,\nDefaultTokens.BOS,\nDefaultTokens.EOS,\n])'}),"\n",(0,s.jsx)(t.p,{children:"When we train a SEQ2SEQ model we use:\nSRC: srctok1 srctok2 srctok3 .... srctokn\nTGT: BOS tgttok1 tgttok2 ..... tgttokm EOS\nBut when training a LM\nSRC: BOS srctok1 srctok2 srctok3 .... srctokn\nTGT: srctok1 srctok2 srctok3 .... srctokn EOS"}),"\n",(0,s.jsx)(t.p,{children:"Having said that, sometimes we need to finetune models (eg: NLLB-200, Llama, ...) with existing vocab\nand special tokens are not the same."}),"\n",(0,s.jsx)(t.p,{children:"ex with NLLB-200\nBOS id=0\nPAD id=1\nEOS id=2\nUNK id=3\nAnd the decoder start token is EOS (</s>) which means in fact that the BOS is never used.\nAt training, TGT needs to start with EOS instead of BOS in the default OpenNMT-py config."}),"\n",(0,s.jsx)(t.p,{children:"Example of Llama\nUNK id=0\nBOS id=1\nEOS id=2\nThere was no PAD but to avoid conflicts we forced PAD id=3 (which was token '<0x00>' in the original llama tokenizer)"})]})}function u(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>r});var s=n(6540);const o={},a=s.createContext(o);function i(e){const t=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(a.Provider,{value:t},e.children)}}}]);