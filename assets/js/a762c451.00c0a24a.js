"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[4132],{2618:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var o=t(4848),a=t(8453);const i={},r="How can I apply on-the-fly tokenization and subword regularization when training?",s={id:"FAQ/tokenization",title:"How can I apply on-the-fly tokenization and subword regularization when training?",description:"This is naturally embedded in the data configuration format introduced in OpenNMT-py 2.0. Each entry of the data configuration will have its own transforms. transforms basically is a list of functions that will be applied sequentially to the examples when read from file.",source:"@site/docs/FAQ/tokenization.md",sourceDirName:"FAQ",slug:"/FAQ/tokenization",permalink:"/eole/docs/FAQ/tokenization",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/tokenization.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"What special tokens are used?",permalink:"/eole/docs/FAQ/special_tokens"},next:{title:"How do I train the Transformer model?",permalink:"/eole/docs/FAQ/transformer"}},l={},d=[{value:"Example",id:"example",level:3}];function c(e){const n={code:"code",h1:"h1",h3:"h3",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"how-can-i-apply-on-the-fly-tokenization-and-subword-regularization-when-training",children:"How can I apply on-the-fly tokenization and subword regularization when training?"}),"\n",(0,o.jsxs)(n.p,{children:["This is naturally embedded in the data configuration format introduced in OpenNMT-py 2.0. Each entry of the ",(0,o.jsx)(n.code,{children:"data"})," configuration will have its own ",(0,o.jsx)(n.code,{children:"transforms"}),". ",(0,o.jsx)(n.code,{children:"transforms"})," basically is a ",(0,o.jsx)(n.code,{children:"list"})," of functions that will be applied sequentially to the examples when read from file."]}),"\n",(0,o.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,o.jsxs)(n.p,{children:["This example applies sentencepiece tokenization with ",(0,o.jsx)(n.code,{children:"pyonmttok"}),", with ",(0,o.jsx)(n.code,{children:"nbest=20"})," and ",(0,o.jsx)(n.code,{children:"alpha=0.1"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# <your_config>.yaml\n\n...\n\n# Tokenization options\nsrc_subword_type: sentencepiece\nsrc_subword_model: examples/subword.spm.model\ntgt_subword_type: sentencepiece\ntgt_subword_model: examples/subword.spm.model\n\n# Number of candidates for SentencePiece sampling\nsubword_nbest: 20\n# Smoothing parameter for SentencePiece sampling\nsubword_alpha: 0.1\n# Specific arguments for pyonmttok\nsrc_onmttok_kwargs: \"{'mode': 'none', 'spacer_annotate': True}\"\ntgt_onmttok_kwargs: \"{'mode': 'none', 'spacer_annotate': True}\"\n\n# Corpus opts:\ndata:\n    corpus_1:\n        path_src: toy-ende/src-train1.txt\n        path_tgt: toy-ende/tgt-train1.txt\n        transforms: [onmt_tokenize]\n        weight: 1\n    valid:\n        path_src: toy-ende/src-val.txt\n        path_tgt: toy-ende/tgt-val.txt\n        transforms: [onmt_tokenize]\n...\n\n"})}),"\n",(0,o.jsx)(n.p,{children:"Other tokenization methods and transforms are readily available. See the dedicated docs for more details."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(6540);const a={},i=o.createContext(a);function r(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);