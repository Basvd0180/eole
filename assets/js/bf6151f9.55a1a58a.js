"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[9953],{9033:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var o=t(4848),i=t(8453);const a={},r="How to use LoRa and 8bit loading to finetune a big model ?",s={id:"FAQ/lora",title:"How to use LoRa and 8bit loading to finetune a big model ?",description:"Cf paper: LoRa",source:"@site/docs/FAQ/lora.md",sourceDirName:"FAQ",slug:"/FAQ/lora",permalink:"/eole/docs/FAQ/lora",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/lora.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How to use gradient checkpointing when dealing with a big model ?",permalink:"/eole/docs/FAQ/gradient_checkpointing"},next:{title:"Performance tips",permalink:"/eole/docs/FAQ/performance"}},l={},d=[];function c(e){const n={a:"a",code:"code",h1:"h1",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"how-to-use-lora-and-8bit-loading-to-finetune-a-big-model-",children:"How to use LoRa and 8bit loading to finetune a big model ?"}),"\n",(0,o.jsxs)(n.p,{children:["Cf paper: ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2106.09685",children:"LoRa"})]}),"\n",(0,o.jsx)(n.p,{children:"LoRa is a mechanism that helps to finetune bigger model on a single GPU card by limiting the anmount of VRAM needed.\nThe principle is to make only a few layers trainable (hence reducing the amount of required memory especially for the Adam optimizer)"}),"\n",(0,o.jsx)(n.p,{children:"You need to train_from a model (for instance NLLB-200 3.3B) and use the following options:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"lora_layers: ['linear_values', 'linear_query']"})," these are the two layers of the Self-Attention module the paper recommend to make trainable."]}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"lora_rank: 2"})}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"lora_dropout: 0.1"})," or any value you can test"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"lora_alpha: 1"})," or any value you can test"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"lora_embedding: true"})," makes Embeddings LoRa compatible, hence trainable in the case you use ",(0,o.jsx)(n.code,{children:"update_vocab: true"})," or if you want to finetune Embeddings as well."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Bitsandbytes enables quantization of Linear layers. For more information: ",(0,o.jsx)(n.a,{href:"https://github.com/TimDettmers/bitsandbytes",children:"https://github.com/TimDettmers/bitsandbytes"}),"\nAlso you can read the blog post here: ",(0,o.jsx)(n.a,{href:"https://huggingface.co/blog/hf-bitsandbytes-integration",children:"https://huggingface.co/blog/hf-bitsandbytes-integration"})]}),"\n",(0,o.jsx)(n.p,{children:"You need to add the following option:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"quant_layers: ['w_1', 'w_2', 'linear_values', 'linear_query']"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"quant_type: ['bnb_NF4']"})}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'You can for instane quantize the layers of the PositionWise Feed-Forward from the Encoder/Decoder and the key/query/values/final from the Multi-head attention.\nChoices for quantization are ["bnb_8bit", "bnb_FP4", "bnb_NF4"]'})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(6540);const i={},a=o.createContext(i);function r(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);