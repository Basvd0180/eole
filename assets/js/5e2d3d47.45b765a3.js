"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[4033],{6534:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var a=t(4848),i=t(8453);const r={},s="Can I get word alignments while translating?",l={id:"FAQ/word_alignments",title:"Can I get word alignments while translating?",description:"Raw alignments from averaging Transformer attention heads",source:"@site/docs/FAQ/word_alignments.md",sourceDirName:"FAQ",slug:"/FAQ/word_alignments",permalink:"/eole/docs/FAQ/word_alignments",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/word_alignments.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How can I update a checkpoint's vocabulary?",permalink:"/eole/docs/FAQ/update_vocab"},next:{title:"Recipes",permalink:"/eole/docs/category/recipes"}},o={},d=[{value:"Raw alignments from averaging Transformer attention heads",id:"raw-alignments-from-averaging-transformer-attention-heads",level:3},{value:"Supervised learning on a specific head",id:"supervised-learning-on-a-specific-head",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",sub:"sub",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"can-i-get-word-alignments-while-translating",children:"Can I get word alignments while translating?"}),"\n",(0,a.jsx)(n.h3,{id:"raw-alignments-from-averaging-transformer-attention-heads",children:"Raw alignments from averaging Transformer attention heads"}),"\n",(0,a.jsxs)(n.p,{children:["Currently, we support producing word alignment while translating for Transformer based models. Using ",(0,a.jsx)(n.code,{children:"-report_align"})," when calling ",(0,a.jsx)(n.code,{children:"translate.py"})," will output the inferred alignments in Pharaoh format. Those alignments are computed from an argmax on the average of the attention heads of the ",(0,a.jsx)(n.em,{children:"second to last"})," decoder layer. The resulting alignment src-tgt (Pharaoh) will be pasted to the translation sentence, separated by ",(0,a.jsx)(n.code,{children:"|||"}),".\nNote: The ",(0,a.jsx)(n.em,{children:"second to last"})," default behaviour was empirically determined. It is not the same as the paper (they take the ",(0,a.jsx)(n.em,{children:"penultimate"})," layer), probably because of slight differences in the architecture."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:['alignments use the standard "Pharaoh format", where a pair ',(0,a.jsx)(n.code,{children:"i-j"})," indicates the i",(0,a.jsx)(n.sub,{children:"th"})," word of source language is aligned to j",(0,a.jsx)(n.sub,{children:"th"})," word of target language."]}),"\n",(0,a.jsx)(n.li,{children:"Example: {'src': 'das stimmt nicht !'; 'output': 'that is not true ! ||| 0-0 0-1 1-2 2-3 1-4 1-5 3-6'}"}),"\n",(0,a.jsxs)(n.li,{children:["Using ",(0,a.jsx)(n.code,{children:"-tgt"})," and ",(0,a.jsx)(n.code,{children:"-gold_align"})," options when calling ",(0,a.jsx)(n.code,{children:"translate.py"}),", we output alignments between the source and the gold target rather than the inferred target, assuming we're doing evaluation."]}),"\n",(0,a.jsxs)(n.li,{children:["To convert subword alignments to word alignments, or symetrize bidirectional alignments, please refer to the ",(0,a.jsx)(n.a,{href:"https://github.com/lilt/alignment-scripts",children:"lilt scripts"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"supervised-learning-on-a-specific-head",children:"Supervised learning on a specific head"}),"\n",(0,a.jsxs)(n.p,{children:["The quality of output alignments can be further improved by providing reference alignments while training. This will invoke multi-task learning on translation and alignment. This is an implementation based on the paper ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/1909.02074",children:"Jointly Learning to Align and Translate with Transformer Models"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["The data need to be preprocessed with the reference alignments in order to learn the supervised task.\nThe reference alignment file(s) can for instance be generated by ",(0,a.jsx)(n.a,{href:"https://github.com/moses-smt/mgiza/",children:"GIZA++"})," or ",(0,a.jsx)(n.a,{href:"https://github.com/clab/fast_align",children:"fast_align"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"In order to learn the supervised task, you can set for each dataset the path of its alignment file in the YAML configuration file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"<your_config>.yaml\n\n...\n\n# Corpus opts:\ndata:\n    corpus_1:\n        path_src: toy-ende/src-train1.txt\n        path_tgt: toy-ende/tgt-train1.txt\n        # src - tgt alignments in pharaoh format\n        path_align: toy-ende/src-tgt.align\n        transforms: []\n        weight: 1\n    valid:\n        path_src: toy-ende/src-val.txt\n        path_tgt: toy-ende/tgt-val.txt\n        transforms: []\n\n...\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Notes"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Most of the transforms are for now incompatible with the joint alignment learning pipeline, because most of them make modifications at the token level, hence alignments would be made invalid."}),"\n",(0,a.jsx)(n.li,{children:"There should be no blank lines in the alignment files provided."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Training options to learn such alignments are:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-lambda_align"}),": set the value > 0.0 to enable joint align training, the paper suggests 0.05;"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-alignment_layer"}),": indicate the index of the decoder layer;"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-alignment_heads"}),":  number of alignment heads for the alignment task - should be set to 1 for the supervised task, and preferably kept to default (or same as ",(0,a.jsx)(n.code,{children:"num_heads"}),") for the average task;"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"-full_context_alignment"}),": do full context decoder pass (no future mask) when computing alignments. This will slow down the training (~12% in terms of tok/s) but will be beneficial to generate better alignment."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var a=t(6540);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);