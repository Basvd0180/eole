"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[3377],{1800:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>a,frontMatter:()=>r,metadata:()=>d,toc:()=>u});var t=s(4848),o=s(8453);const r={},i="Do you support multi-gpu?",d={id:"FAQ/distributed",title:"Do you support multi-gpu?",description:"First you need to make sure you export CUDAVISIBLEDEVICES=0,1,2,3.",source:"@site/docs/FAQ/distributed.md",sourceDirName:"FAQ",slug:"/FAQ/distributed",permalink:"/eole/docs/FAQ/distributed",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/distributed.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How can I create custom on-the-fly data transforms?",permalink:"/eole/docs/FAQ/custom_transforms"},next:{title:"How can I ensemble Models at inference?",permalink:"/eole/docs/FAQ/ensemble_decoding"}},c={},u=[];function l(e){const n={code:"code",h1:"h1",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"do-you-support-multi-gpu",children:"Do you support multi-gpu?"}),"\n",(0,t.jsxs)(n.p,{children:["First you need to make sure you ",(0,t.jsx)(n.code,{children:"export CUDA_VISIBLE_DEVICES=0,1,2,3"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["If you want to use GPU id 1 and 3 of your OS, you will need to ",(0,t.jsx)(n.code,{children:"export CUDA_VISIBLE_DEVICES=1,3"})]}),"\n",(0,t.jsxs)(n.p,{children:["Both ",(0,t.jsx)(n.code,{children:"-world_size"})," and ",(0,t.jsx)(n.code,{children:"-gpu_ranks"})," need to be set. E.g. ",(0,t.jsx)(n.code,{children:"-world_size 4 -gpu_ranks 0 1 2 3"})," will use 4 GPU on this node only."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Warning - Deprecated"})}),"\n",(0,t.jsx)(n.p,{children:"Multi-node distributed training has not been properly re-implemented since OpenNMT-py 2.0."}),"\n",(0,t.jsxs)(n.p,{children:["If you want to use 2 nodes with 2 GPU each, you need to set ",(0,t.jsx)(n.code,{children:"-master_ip"})," and ",(0,t.jsx)(n.code,{children:"-master_port"}),", and"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"-world_size 4 -gpu_ranks 0 1"}),": on the first node"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"-world_size 4 -gpu_ranks 2 3"}),": on the second node"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"-accum_count 2"}),": This will accumulate over 2 batches before updating parameters."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["If you use a regular network card (1 Gbps) then we suggest to use a higher ",(0,t.jsx)(n.code,{children:"-accum_count"})," to minimize the inter-node communication."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Note:"})}),"\n",(0,t.jsxs)(n.p,{children:["In the legacy version, when training on several GPUs, you couldn't have them in 'Exclusive' compute mode (",(0,t.jsx)(n.code,{children:"nvidia-smi -c 3"}),")."]}),"\n",(0,t.jsxs)(n.p,{children:["The multi-gpu setup relied on a Producer/Consumer setup. This setup means there will be ",(0,t.jsx)(n.code,{children:"2<n_gpu> + 1"})," processes spawned, with 2 processes per GPU, one for model training and one (Consumer) that hosts a ",(0,t.jsx)(n.code,{children:"Queue"})," of batches that will be processed next. The additional process is the Producer, creating batches and sending them to the Consumers. This setup is beneficial for both wall time and memory, since it loads data shards 'in advance', and does not require to load it for each GPU process."]}),"\n",(0,t.jsx)(n.p,{children:"The new codebase allows GPUs to be in exclusive mode, because batches are moved to the device later in the process. Hence, there is no 'producer' process on each GPU."})]})}function a(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>d});var t=s(6540);const o={},r=t.createContext(o);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);