<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-reference/Config/training" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.2.0">
<title data-rh="true">Training | Eole - üë∑‚Äç‚ôÇÔ∏èüöß Work In Progress</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://eole-nlp.github.io/eole/docs/reference/Config/training"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Training | Eole - üë∑‚Äç‚ôÇÔ∏èüöß Work In Progress"><meta data-rh="true" name="description" content="pydantic model eole.config.training.OptimizerConfig[source]"><meta data-rh="true" property="og:description" content="pydantic model eole.config.training.OptimizerConfig[source]"><link data-rh="true" rel="icon" href="/eole/img/eole-logo.ico"><link data-rh="true" rel="canonical" href="https://eole-nlp.github.io/eole/docs/reference/Config/training"><link data-rh="true" rel="alternate" href="https://eole-nlp.github.io/eole/docs/reference/Config/training" hreflang="en"><link data-rh="true" rel="alternate" href="https://eole-nlp.github.io/eole/docs/reference/Config/training" hreflang="x-default"><link rel="stylesheet" href="/eole/assets/css/styles.0e100862.css">
<script src="/eole/assets/js/runtime~main.4721e053.js" defer="defer"></script>
<script src="/eole/assets/js/main.2974e5ec.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/eole/"><div class="navbar__logo"><img src="/eole/img/eole-logo.png" alt="Eole Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/eole/img/eole-logo.png" alt="Eole Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/eole/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/eole/docs/reference/index">Reference</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/eole-nlp/eole" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/reference/index">Eole Core API</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/eole/docs/reference/Config/">Configuration</a><button aria-label="Collapse sidebar category &#x27;Configuration&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Config/data">Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Config/inference">Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Config/models">Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Config/run">Main Entrypoints</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/eole/docs/reference/Config/training">Training</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/eole/docs/reference/Core API/core">Core API</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/reference/bibliography">Bibliography</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/eole/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/eole/docs/reference/Config/"><span itemprop="name">Configuration</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Training</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Training</h1>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pydantic-model-eoleconfigtrainingoptimizerconfigsource"><em>pydantic model</em> eole.config.training.OptimizerConfig<a href="https://github.com/eole-nlp/eole/blob/master/eole/config/training.py#L10-L82" target="_blank" rel="noopener noreferrer">[source]</a><a href="#pydantic-model-eoleconfigtrainingoptimizerconfigsource" class="hash-link" aria-label="Direct link to pydantic-model-eoleconfigtrainingoptimizerconfigsource" title="Direct link to pydantic-model-eoleconfigtrainingoptimizerconfigsource">‚Äã</a></h3>
<p>Bases: <code>Config</code></p>
<p>Everything related to optimizers.
Might be split into multiple subclasses later.
Note: not fully sufficient (yet) to replace full opt namespace in build_torch_optimizer.
Some other parameters (hidden_size, compute_dtype, apex_opt_level, etc.) are accessed.</p>
<p></p><details class="details_lb9f alert alert--info details_b_Ee autodoc_pydantic_collapsable_json" data-collapsed="true"><summary>Show JSON schema</summary><div><div class="collapsibleContent_i85q">
<!-- -->
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;title&quot;: &quot;OptimizerConfig&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;description&quot;: &quot;Everything related to optimizers.\nMight be split into multiple subclasses later.\nNote: not fully sufficient (yet) to replace full opt namespace in build_torch_optimizer.\nSome other parameters (hidden_size, compute_dtype, apex_opt_level, etc.) are accessed.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;type&quot;: &quot;object&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;optim&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;sgd&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Optimization method.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;sgd&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adagrad&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adadelta&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;sparseadam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adafactor&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;fusedadam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adamw8bit&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;pagedadamw8bit&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;pagedadamw32bit&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Optim&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;adagrad_accumulator_init&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Adagrad Accumulator Init&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;adam_beta1&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.9,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Adam Beta1&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;adam_beta2&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.999,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and Keras (https://keras.io/optimizers/). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Adam Beta2&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;learning_rate&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Learning Rate&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;learning_rate_decay&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.5,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Decay learning rate by this much if steps have gone past start_decay_steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Learning Rate Decay&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;start_decay_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 50000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Start decaying every decay_steps after this many steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Start Decay Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;decay_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 10000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Frequency for learning rate decay, in steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Decay Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;decay_method&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;none&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Custom decay method to use.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;noam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;noamwd&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;rsqrt&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;none&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Decay Method&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;warmup_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 4000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of warmup steps for custom decay.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Warmup Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;reset_optim&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;none&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Optimization resetter when using train_from.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;none&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;all&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;states&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;keep_states&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Reset Optim&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;additionalProperties&quot;: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
</div></div></details><p></p>
<ul>
<li><strong>Config:</strong>
<ul>
<li><strong>validate_assignment</strong>: <em>bool = True</em></li>
<li><strong>validate_default</strong>: <em>bool = True</em></li>
<li><strong>use_enum_values</strong>: <em>bool = True</em></li>
<li><strong>extra</strong>: <em>str = forbid</em></li>
<li><strong>protected_namespaces</strong>: <em>tuple = ()</em></li>
</ul>
</li>
<li><strong>Fields:</strong>
<ul>
<li><a href="#eole.config.training.OptimizerConfig.adagrad_accumulator_init"><code>adagrad_accumulator_init (float)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.adam_beta1"><code>adam_beta1 (float)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.adam_beta2"><code>adam_beta2 (float)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.decay_method"><code>decay_method (Literal[&#x27;noam&#x27;, &#x27;noamwd&#x27;, &#x27;rsqrt&#x27;, &#x27;none&#x27;])</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.decay_steps"><code>decay_steps (int)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.learning_rate"><code>learning_rate (float)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.learning_rate_decay"><code>learning_rate_decay (float)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.optim"><code>optim (Literal[&#x27;sgd&#x27;, &#x27;adagrad&#x27;, &#x27;adadelta&#x27;, &#x27;adam&#x27;, &#x27;sparseadam&#x27;, &#x27;adafactor&#x27;, &#x27;fusedadam&#x27;, &#x27;adamw8bit&#x27;, &#x27;pagedadamw8bit&#x27;, &#x27;pagedadamw32bit&#x27;])</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.reset_optim"><code>reset_optim (Literal[&#x27;none&#x27;, &#x27;all&#x27;, &#x27;states&#x27;, &#x27;keep_states&#x27;])</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.start_decay_steps"><code>start_decay_steps (int)</code></a></li>
<li><a href="#eole.config.training.OptimizerConfig.warmup_steps"><code>warmup_steps (int)</code></a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-adagrad_accumulator_init--float--0"><em>field</em> adagrad_accumulator_init <em>: float</em> <em>= 0</em><a href="#field-adagrad_accumulator_init--float--0" class="hash-link" aria-label="Direct link to field-adagrad_accumulator_init--float--0" title="Direct link to field-adagrad_accumulator_init--float--0">‚Äã</a></h4>
<p>Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-adam_beta1--float--09"><em>field</em> adam_beta1 <em>: float</em> <em>= 0.9</em><a href="#field-adam_beta1--float--09" class="hash-link" aria-label="Direct link to field-adam_beta1--float--09" title="Direct link to field-adam_beta1--float--09">‚Äã</a></h4>
<p>Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-adam_beta2--float--0999"><em>field</em> adam_beta2 <em>: float</em> <em>= 0.999</em><a href="#field-adam_beta2--float--0999" class="hash-link" aria-label="Direct link to field-adam_beta2--float--0999" title="Direct link to field-adam_beta2--float--0999">‚Äã</a></h4>
<p>Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" target="_blank" rel="noopener noreferrer">https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer</a>) and Keras (<a href="https://keras.io/optimizers/" target="_blank" rel="noopener noreferrer">https://keras.io/optimizers/</a>). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-decay_method--literalnoam-noamwd-rsqrt-none--none"><em>field</em> decay_method <em>: Literal[&#x27;noam&#x27;, &#x27;noamwd&#x27;, &#x27;rsqrt&#x27;, &#x27;none&#x27;]</em> <em>= &#x27;none&#x27;</em><a href="#field-decay_method--literalnoam-noamwd-rsqrt-none--none" class="hash-link" aria-label="Direct link to field-decay_method--literalnoam-noamwd-rsqrt-none--none" title="Direct link to field-decay_method--literalnoam-noamwd-rsqrt-none--none">‚Äã</a></h4>
<p>Custom decay method to use.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-decay_steps--int--10000"><em>field</em> decay_steps <em>: int</em> <em>= 10000</em><a href="#field-decay_steps--int--10000" class="hash-link" aria-label="Direct link to field-decay_steps--int--10000" title="Direct link to field-decay_steps--int--10000">‚Äã</a></h4>
<p>Frequency for learning rate decay, in steps.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-learning_rate--float--10"><em>field</em> learning_rate <em>: float</em> <em>= 1.0</em><a href="#field-learning_rate--float--10" class="hash-link" aria-label="Direct link to field-learning_rate--float--10" title="Direct link to field-learning_rate--float--10">‚Äã</a></h4>
<p>Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-learning_rate_decay--float--05"><em>field</em> learning_rate_decay <em>: float</em> <em>= 0.5</em><a href="#field-learning_rate_decay--float--05" class="hash-link" aria-label="Direct link to field-learning_rate_decay--float--05" title="Direct link to field-learning_rate_decay--float--05">‚Äã</a></h4>
<p>Decay learning rate by this much if steps have gone past start_decay_steps.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-optim--literalsgd-adagrad-adadelta-adam-sparseadam-adafactor-fusedadam-adamw8bit-pagedadamw8bit-pagedadamw32bit--sgd"><em>field</em> optim <em>: Literal[&#x27;sgd&#x27;, &#x27;adagrad&#x27;, &#x27;adadelta&#x27;, &#x27;adam&#x27;, &#x27;sparseadam&#x27;, &#x27;adafactor&#x27;, &#x27;fusedadam&#x27;, &#x27;adamw8bit&#x27;, &#x27;pagedadamw8bit&#x27;, &#x27;pagedadamw32bit&#x27;]</em> <em>= &#x27;sgd&#x27;</em><a href="#field-optim--literalsgd-adagrad-adadelta-adam-sparseadam-adafactor-fusedadam-adamw8bit-pagedadamw8bit-pagedadamw32bit--sgd" class="hash-link" aria-label="Direct link to field-optim--literalsgd-adagrad-adadelta-adam-sparseadam-adafactor-fusedadam-adamw8bit-pagedadamw8bit-pagedadamw32bit--sgd" title="Direct link to field-optim--literalsgd-adagrad-adadelta-adam-sparseadam-adafactor-fusedadam-adamw8bit-pagedadamw8bit-pagedadamw32bit--sgd">‚Äã</a></h4>
<p>Optimization method.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-reset_optim--literalnone-all-states-keep_states--none"><em>field</em> reset_optim <em>: Literal[&#x27;none&#x27;, &#x27;all&#x27;, &#x27;states&#x27;, &#x27;keep_states&#x27;]</em> <em>= &#x27;none&#x27;</em><a href="#field-reset_optim--literalnone-all-states-keep_states--none" class="hash-link" aria-label="Direct link to field-reset_optim--literalnone-all-states-keep_states--none" title="Direct link to field-reset_optim--literalnone-all-states-keep_states--none">‚Äã</a></h4>
<p>Optimization resetter when using train_from.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-start_decay_steps--int--50000"><em>field</em> start_decay_steps <em>: int</em> <em>= 50000</em><a href="#field-start_decay_steps--int--50000" class="hash-link" aria-label="Direct link to field-start_decay_steps--int--50000" title="Direct link to field-start_decay_steps--int--50000">‚Äã</a></h4>
<p>Start decaying every decay_steps after this many steps.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-warmup_steps--int--4000"><em>field</em> warmup_steps <em>: int</em> <em>= 4000</em><a href="#field-warmup_steps--int--4000" class="hash-link" aria-label="Direct link to field-warmup_steps--int--4000" title="Direct link to field-warmup_steps--int--4000">‚Äã</a></h4>
<p>Number of warmup steps for custom decay.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pydantic-model-eoleconfigtrainingtrainingconfigsource"><em>pydantic model</em> eole.config.training.TrainingConfig<a href="https://github.com/eole-nlp/eole/blob/master/eole/config/training.py#L85-L360" target="_blank" rel="noopener noreferrer">[source]</a><a href="#pydantic-model-eoleconfigtrainingtrainingconfigsource" class="hash-link" aria-label="Direct link to pydantic-model-eoleconfigtrainingtrainingconfigsource" title="Direct link to pydantic-model-eoleconfigtrainingtrainingconfigsource">‚Äã</a></h3>
<p>Bases: <code>RunningConfig</code>, <a href="#eole.config.training.OptimizerConfig"><code>OptimizerConfig</code></a>, <a href="/eole/docs/reference/Config/run#eole.config.common.LoRaConfig"><code>LoRaConfig</code></a>, <a href="/eole/docs/reference/Config/run#eole.config.common.QuantizeConfig"><code>QuantizeConfig</code></a></p>
<p></p><details class="details_lb9f alert alert--info details_b_Ee autodoc_pydantic_collapsable_json" data-collapsed="true"><summary>Show JSON schema</summary><div><div class="collapsibleContent_i85q">
<!-- -->
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;title&quot;: &quot;TrainingConfig&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;type&quot;: &quot;object&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;properties&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;quant_layers&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Quant Layers&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;quant_type&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Type of compression.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;bnb_9bit&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;bnb_FP4&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;bnb_NF4&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;awq_gemm&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;awq_gemv&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Quant Type&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;w_bit&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 4,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;W_bit quantization&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;W Bit&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;group_size&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 128,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Group size quantization.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Group Size&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lora_layers&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. [&#x27;linear_values&#x27;, &#x27;linear_query&#x27;] (\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lora Layers&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lora_embedding&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\u00a75.1)&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lora Embedding&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lora_rank&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 2,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lora Rank&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lora_alpha&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;\u00a74.1 https://arxiv.org/abs/2106.09685&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lora Alpha&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lora_dropout&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lora Dropout&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;optim&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;sgd&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Optimization method.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;sgd&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adagrad&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adadelta&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;sparseadam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adafactor&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;fusedadam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;adamw8bit&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;pagedadamw8bit&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;pagedadamw32bit&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Optim&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;adagrad_accumulator_init&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Adagrad Accumulator Init&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;adam_beta1&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.9,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Adam Beta1&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;adam_beta2&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.999,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and Keras (https://keras.io/optimizers/). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Adam Beta2&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;learning_rate&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Learning Rate&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;learning_rate_decay&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.5,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Decay learning rate by this much if steps have gone past start_decay_steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Learning Rate Decay&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;start_decay_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 50000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Start decaying every decay_steps after this many steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Start Decay Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;decay_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 10000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Frequency for learning rate decay, in steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Decay Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;decay_method&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;none&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Custom decay method to use.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;noam&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;noamwd&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;rsqrt&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;none&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Decay Method&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;warmup_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 4000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of warmup steps for custom decay.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Warmup Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;reset_optim&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;none&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Optimization resetter when using train_from.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;none&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;all&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;states&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;keep_states&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Reset Optim&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;gpu_ranks&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;List of ranks for each process.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Gpu Ranks&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;world_size&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Total number of distributed processes.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;World Size&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;parallel_mode&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;data_parallel&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Distributed mode.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;data_parallel&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;tensor_parallel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Parallel Mode&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;gpu_backend&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;nccl&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Type of torch distributed backend.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Gpu Backend&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;gpu_verbose_level&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Gpu Verbose Level&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;master_ip&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;localhost&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Master Ip&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;master_port&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 10000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Master Port&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;timeout&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 60,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Timeout&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;model_path&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;model&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Path to directory containing all model components.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Model Path&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;self_attn_backend&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;flash&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Self-attention backend.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;flash&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;pytorch&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Self Attn Backend&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;compute_dtype&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;fp32&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;fp16&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;int8&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;bf16&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Compute Dtype&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;param_init&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Support value for uniform distribution parameters initialization. Set to 0 not to use initialization.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Param Init&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;param_init_glorot&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Initialize parameters with xavier_uniform. Required for transformer.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Param Init Glorot&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;freeze_encoder&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Freeze parameters in encoder.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Freeze Encoder&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;freeze_decoder&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Freeze parameters in decoder.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Freeze Decoder&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;pre_word_vecs_enc&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;anyOf&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;null&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;If a valid path is specified, will load pretrained word embeddings on the encoder side.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Pre Word Vecs Enc&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;pre_word_vecs_dec&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;anyOf&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;null&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;If a valid path is specified, will load pretrained word embeddings on the decoder side.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Pre Word Vecs Dec&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;data_type&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;anyOf&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;null&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;text&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Data Type&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;bucket_size&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 262144,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;A bucket is a buffer of bucket_size examples to pick from the various corpora. The dynamic iterator batches batch_size items from the bucket and shuffle them.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Bucket Size&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;bucket_size_init&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: -1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Bucket size is initialized with this amount of examples (see bucket_size_increment).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Bucket Size Init&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;bucket_size_increment&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Bucket size incremented with this amount of examples at each new bucket (up to bucket_size).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Bucket Size Increment&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;prefetch_factor&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 200,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of mini-batches loaded in advance to avoid the GPU waiting during processing of next bucket.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Prefetch Factor&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;save_format&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;pytorch&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Format to save the model weights.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;pytorch&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;safetensors&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Save Format&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;save_checkpoint_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 5000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Frequency of checkpoint saving (in steps).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Save Checkpoint Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;keep_checkpoint&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: -1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of checkpoints to retain. (-1 retains all)&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Keep Checkpoint&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;train_from&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;anyOf&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;null&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Pretrained model/checkpoint weights to continue training from.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Train From&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;num_workers&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 2,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of workers for pytorch.DataLoader objects.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Num Workers&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;batch_size&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 64,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Maximum batch size for training.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Batch Size&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;batch_size_multiple&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Batch size multiple for token batches.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Batch Size Multiple&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;batch_type&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;sents&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Batch grouping for batch_size.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;sents&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;tokens&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Batch Type&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;normalization&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;sents&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Normalization method of the gradient.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;sents&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;tokens&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Normalization&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;accum_count&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once. Recommended for transformer.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Accum Count&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;accum_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Steps at which accum_count values change.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Accum Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;valid_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 10000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Frequency of validation, in steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Valid Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;valid_batch_size&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 32,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Maximum batch size for validation.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Valid Batch Size&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;train_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 100000,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of training steps.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Train Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;single_pass&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Make a single pass over the training dataset.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Single Pass&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;early_stopping&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Number of validation steps without improving that will trigger early stop of training.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Early Stopping&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;early_stopping_criteria&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;anyOf&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;null&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Criteria to use for early stopping.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Early Stopping Criteria&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;max_grad_norm&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 5,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;If the norm of the gradient vector exceeds this value, renormalize it to have the norm equal to max_grad_norm.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Max Grad Norm&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;dropout&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            0.3</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Dropout probability.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Dropout&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;attention_dropout&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            0.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Attention dropout probability.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Attention Dropout&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;dropout_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Steps at which dropout changes.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Dropout Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;truncated_decoder&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Truncated bptt.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Truncated Decoder&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;label_smoothing&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Label smoothing value epsilon. Probability of all non-true labels will be smoothed by epsilon/(vocab_size-1). Set to 0 to turn off label smoothing. (https://arxiv.org/abs/1512.00567)&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Label Smoothing&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;average_decay&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Exponential moving average decay (https://en.wikipedia.org/wiki/Moving_average). Set to other than 0 (e.g. 1e-4) to activate. Similar to Marian NMT implementation (http://www.aclweb.org/anthology/P18-4020).&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Average Decay&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;average_every&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Step for moving average. Default is every update if average_decay is set.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Average Every&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;loss_scale&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;For FP16 training, the static loss scale to use. If not set, the loss scale is dynamically computed.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Loss Scale&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;apex_opt_level&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: &quot;&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;For FP16 training, the opt_level to use. See https://nvidia.github.io/apex/amp.html#opt-levels.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;enum&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;O0&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;O1&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;O2&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;O3&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Apex Opt Level&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;zero_out_prompt_loss&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Set the prompt loss to zero. Mostly for LLM finetuning. Will be enabled only if the `insert_mask_before_placeholder` transform is applied.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Zero Out Prompt Loss&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;use_ckpting&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Use gradient checkpointing for those modules.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Use Ckpting&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;update_vocab&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: false,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Update source and target existing vocabularies.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Update Vocab&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;boolean&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lm_prior_model&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;anyOf&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;string&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               &quot;type&quot;: &quot;null&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;LM model to use to train the TM.&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lm Prior Model&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lm_prior_lambda&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;LM Prior Lambda&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lm Prior Lambda&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;lm_prior_tau&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 1.0,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;LM Prior Tau&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Lm Prior Tau&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;estim_loss_lambda&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            1.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Weight applied to estimator loss&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Estim Loss Lambda&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;estim_loss_lambda_steps&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: [</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         ],</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Steps at which estimator loss lambda changes&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;items&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            &quot;type&quot;: &quot;integer&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Estim Loss Lambda Steps&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;array&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &quot;score_threshold&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;default&quot;: 0.68,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;description&quot;: &quot;Threshold to filterout data&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;title&quot;: &quot;Score Threshold&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         &quot;type&quot;: &quot;number&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   },</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   &quot;additionalProperties&quot;: false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
</div></div></details><p></p>
<ul>
<li><strong>Config:</strong>
<ul>
<li><strong>validate_assignment</strong>: <em>bool = True</em></li>
<li><strong>validate_default</strong>: <em>bool = True</em></li>
<li><strong>use_enum_values</strong>: <em>bool = True</em></li>
<li><strong>extra</strong>: <em>str = forbid</em></li>
<li><strong>protected_namespaces</strong>: <em>tuple = ()</em></li>
<li><strong>arbitrary_types_allowed</strong>: <em>bool = True</em></li>
</ul>
</li>
<li><strong>Fields:</strong>
<ul>
<li><a href="#eole.config.training.TrainingConfig.accum_count"><code>accum_count (List[int])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.accum_steps"><code>accum_steps (List[int])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.apex_opt_level"><code>apex_opt_level (Literal[&#x27;&#x27;, &#x27;O0&#x27;, &#x27;O1&#x27;, &#x27;O2&#x27;, &#x27;O3&#x27;])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.attention_dropout"><code>attention_dropout (List[float])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.average_decay"><code>average_decay (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.average_every"><code>average_every (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.batch_size"><code>batch_size (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.batch_size_multiple"><code>batch_size_multiple (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.batch_type"><code>batch_type (Literal[&#x27;sents&#x27;, &#x27;tokens&#x27;])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.bucket_size"><code>bucket_size (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.bucket_size_increment"><code>bucket_size_increment (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.bucket_size_init"><code>bucket_size_init (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.data_type"><code>data_type (str | None)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.dropout"><code>dropout (List[float])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.dropout_steps"><code>dropout_steps (List[int])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.early_stopping"><code>early_stopping (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.early_stopping_criteria"><code>early_stopping_criteria (str | None)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.estim_loss_lambda"><code>estim_loss_lambda (List[float])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.estim_loss_lambda_steps"><code>estim_loss_lambda_steps (List[int])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.freeze_decoder"><code>freeze_decoder (bool)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.freeze_encoder"><code>freeze_encoder (bool)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.keep_checkpoint"><code>keep_checkpoint (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.label_smoothing"><code>label_smoothing (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.lm_prior_lambda"><code>lm_prior_lambda (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.lm_prior_model"><code>lm_prior_model (str | None)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.lm_prior_tau"><code>lm_prior_tau (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.loss_scale"><code>loss_scale (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.max_grad_norm"><code>max_grad_norm (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.normalization"><code>normalization (Literal[&#x27;sents&#x27;, &#x27;tokens&#x27;])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.num_workers"><code>num_workers (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.param_init"><code>param_init (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.param_init_glorot"><code>param_init_glorot (bool)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.pre_word_vecs_dec"><code>pre_word_vecs_dec (str | None)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.pre_word_vecs_enc"><code>pre_word_vecs_enc (str | None)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.prefetch_factor"><code>prefetch_factor (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.save_checkpoint_steps"><code>save_checkpoint_steps (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.save_format"><code>save_format (Literal[&#x27;pytorch&#x27;, &#x27;safetensors&#x27;])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.score_threshold"><code>score_threshold (float)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.single_pass"><code>single_pass (bool)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.train_from"><code>train_from (str | None)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.train_steps"><code>train_steps (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.truncated_decoder"><code>truncated_decoder (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.update_vocab"><code>update_vocab (bool)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.use_ckpting"><code>use_ckpting (List[str])</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.valid_batch_size"><code>valid_batch_size (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.valid_steps"><code>valid_steps (int)</code></a></li>
<li><a href="#eole.config.training.TrainingConfig.zero_out_prompt_loss"><code>zero_out_prompt_loss (bool)</code></a></li>
</ul>
</li>
<li><strong>Validators:</strong>
<ul>
<li><code>_validate_running_config</code> ¬ª <code>all fields</code></li>
<li><a href="#eole.config.training.TrainingConfig.checkpointing_layers"><code>checkpointing_layers</code></a> ¬ª <a href="#eole.config.training.TrainingConfig.use_ckpting"><code>use_ckpting</code></a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-accum_count--listint--1"><em>field</em> accum_count <em>: List[int]</em> <em>= [1]</em><a href="#field-accum_count--listint--1" class="hash-link" aria-label="Direct link to field-accum_count--listint--1" title="Direct link to field-accum_count--listint--1">‚Äã</a></h4>
<p>Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once. Recommended for transformer.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-accum_steps--listint--0"><em>field</em> accum_steps <em>: List[int]</em> <em>= [0]</em><a href="#field-accum_steps--listint--0" class="hash-link" aria-label="Direct link to field-accum_steps--listint--0" title="Direct link to field-accum_steps--listint--0">‚Äã</a></h4>
<p>Steps at which accum_count values change.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-apex_opt_level--literal-o0-o1-o2-o3--"><em>field</em> apex_opt_level <em>: Literal[&#x27;&#x27;, &#x27;O0&#x27;, &#x27;O1&#x27;, &#x27;O2&#x27;, &#x27;O3&#x27;]</em> <em>= &#x27;&#x27;</em><a href="#field-apex_opt_level--literal-o0-o1-o2-o3--" class="hash-link" aria-label="Direct link to field-apex_opt_level--literal-o0-o1-o2-o3--" title="Direct link to field-apex_opt_level--literal-o0-o1-o2-o3--">‚Äã</a></h4>
<p>For FP16 training, the opt_level to use. See <a href="https://nvidia.github.io/apex/amp.html#opt-levels" target="_blank" rel="noopener noreferrer">https://nvidia.github.io/apex/amp.html#opt-levels</a>.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-attention_dropout--listfloat--01"><em>field</em> attention_dropout <em>: List[float]</em> <em>= [0.1]</em><a href="#field-attention_dropout--listfloat--01" class="hash-link" aria-label="Direct link to field-attention_dropout--listfloat--01" title="Direct link to field-attention_dropout--listfloat--01">‚Äã</a></h4>
<p>Attention dropout probability.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-average_decay--float--00"><em>field</em> average_decay <em>: float</em> <em>= 0.0</em><a href="#field-average_decay--float--00" class="hash-link" aria-label="Direct link to field-average_decay--float--00" title="Direct link to field-average_decay--float--00">‚Äã</a></h4>
<p>Exponential moving average decay (<a href="https://en.wikipedia.org/wiki/Moving_average" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Moving_average</a>). Set to other than 0 (e.g. 1e-4) to activate. Similar to Marian NMT implementation (<a href="http://www.aclweb.org/anthology/P18-4020" target="_blank" rel="noopener noreferrer">http://www.aclweb.org/anthology/P18-4020</a>).</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-average_every--int--1"><em>field</em> average_every <em>: int</em> <em>= 1</em><a href="#field-average_every--int--1" class="hash-link" aria-label="Direct link to field-average_every--int--1" title="Direct link to field-average_every--int--1">‚Äã</a></h4>
<p>Step for moving average. Default is every update if average_decay is set.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-batch_size--int--64"><em>field</em> batch_size <em>: int</em> <em>= 64</em><a href="#field-batch_size--int--64" class="hash-link" aria-label="Direct link to field-batch_size--int--64" title="Direct link to field-batch_size--int--64">‚Äã</a></h4>
<p>Maximum batch size for training.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-batch_size_multiple--int--1"><em>field</em> batch_size_multiple <em>: int</em> <em>= 1</em><a href="#field-batch_size_multiple--int--1" class="hash-link" aria-label="Direct link to field-batch_size_multiple--int--1" title="Direct link to field-batch_size_multiple--int--1">‚Äã</a></h4>
<p>Batch size multiple for token batches.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-batch_type--literalsents-tokens--sents"><em>field</em> batch_type <em>: Literal[&#x27;sents&#x27;, &#x27;tokens&#x27;]</em> <em>= &#x27;sents&#x27;</em><a href="#field-batch_type--literalsents-tokens--sents" class="hash-link" aria-label="Direct link to field-batch_type--literalsents-tokens--sents" title="Direct link to field-batch_type--literalsents-tokens--sents">‚Äã</a></h4>
<p>Batch grouping for batch_size.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-bucket_size--int--262144"><em>field</em> bucket_size <em>: int</em> <em>= 262144</em><a href="#field-bucket_size--int--262144" class="hash-link" aria-label="Direct link to field-bucket_size--int--262144" title="Direct link to field-bucket_size--int--262144">‚Äã</a></h4>
<p>A bucket is a buffer of bucket_size examples to pick from the various corpora. The dynamic iterator batches batch_size items from the bucket and shuffle them.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-bucket_size_increment--int--0"><em>field</em> bucket_size_increment <em>: int</em> <em>= 0</em><a href="#field-bucket_size_increment--int--0" class="hash-link" aria-label="Direct link to field-bucket_size_increment--int--0" title="Direct link to field-bucket_size_increment--int--0">‚Äã</a></h4>
<p>Bucket size incremented with this amount of examples at each new bucket (up to bucket_size).</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-bucket_size_init--int---1"><em>field</em> bucket_size_init <em>: int</em> <em>= -1</em><a href="#field-bucket_size_init--int---1" class="hash-link" aria-label="Direct link to field-bucket_size_init--int---1" title="Direct link to field-bucket_size_init--int---1">‚Äã</a></h4>
<p>Bucket size is initialized with this amount of examples (see bucket_size_increment).</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-data_type--str--none--text"><em>field</em> data_type <em>: str | None</em> <em>= &#x27;text&#x27;</em><a href="#field-data_type--str--none--text" class="hash-link" aria-label="Direct link to field-data_type--str--none--text" title="Direct link to field-data_type--str--none--text">‚Äã</a></h4>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-dropout--listfloat--03"><em>field</em> dropout <em>: List[float]</em> <em>= [0.3]</em><a href="#field-dropout--listfloat--03" class="hash-link" aria-label="Direct link to field-dropout--listfloat--03" title="Direct link to field-dropout--listfloat--03">‚Äã</a></h4>
<p>Dropout probability.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-dropout_steps--listint--0"><em>field</em> dropout_steps <em>: List[int]</em> <em>= [0]</em><a href="#field-dropout_steps--listint--0" class="hash-link" aria-label="Direct link to field-dropout_steps--listint--0" title="Direct link to field-dropout_steps--listint--0">‚Äã</a></h4>
<p>Steps at which dropout changes.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-early_stopping--int--0"><em>field</em> early_stopping <em>: int</em> <em>= 0</em><a href="#field-early_stopping--int--0" class="hash-link" aria-label="Direct link to field-early_stopping--int--0" title="Direct link to field-early_stopping--int--0">‚Äã</a></h4>
<p>Number of validation steps without improving that will trigger early stop of training.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-early_stopping_criteria--str--none--none"><em>field</em> early_stopping_criteria <em>: str | None</em> <em>= None</em><a href="#field-early_stopping_criteria--str--none--none" class="hash-link" aria-label="Direct link to field-early_stopping_criteria--str--none--none" title="Direct link to field-early_stopping_criteria--str--none--none">‚Äã</a></h4>
<p>Criteria to use for early stopping.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-estim_loss_lambda--listfloat--10"><em>field</em> estim_loss_lambda <em>: List[float]</em> <em>= [1.0]</em><a href="#field-estim_loss_lambda--listfloat--10" class="hash-link" aria-label="Direct link to field-estim_loss_lambda--listfloat--10" title="Direct link to field-estim_loss_lambda--listfloat--10">‚Äã</a></h4>
<p>Weight applied to estimator loss</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-estim_loss_lambda_steps--listint--0"><em>field</em> estim_loss_lambda_steps <em>: List[int]</em> <em>= [0]</em><a href="#field-estim_loss_lambda_steps--listint--0" class="hash-link" aria-label="Direct link to field-estim_loss_lambda_steps--listint--0" title="Direct link to field-estim_loss_lambda_steps--listint--0">‚Äã</a></h4>
<p>Steps at which estimator loss lambda changes</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-freeze_decoder--bool--false"><em>field</em> freeze_decoder <em>: bool</em> <em>= False</em><a href="#field-freeze_decoder--bool--false" class="hash-link" aria-label="Direct link to field-freeze_decoder--bool--false" title="Direct link to field-freeze_decoder--bool--false">‚Äã</a></h4>
<p>Freeze parameters in decoder.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-freeze_encoder--bool--false"><em>field</em> freeze_encoder <em>: bool</em> <em>= False</em><a href="#field-freeze_encoder--bool--false" class="hash-link" aria-label="Direct link to field-freeze_encoder--bool--false" title="Direct link to field-freeze_encoder--bool--false">‚Äã</a></h4>
<p>Freeze parameters in encoder.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-keep_checkpoint--int---1"><em>field</em> keep_checkpoint <em>: int</em> <em>= -1</em><a href="#field-keep_checkpoint--int---1" class="hash-link" aria-label="Direct link to field-keep_checkpoint--int---1" title="Direct link to field-keep_checkpoint--int---1">‚Äã</a></h4>
<p>Number of checkpoints to retain. (-1 retains all)</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-label_smoothing--float--00"><em>field</em> label_smoothing <em>: float</em> <em>= 0.0</em><a href="#field-label_smoothing--float--00" class="hash-link" aria-label="Direct link to field-label_smoothing--float--00" title="Direct link to field-label_smoothing--float--00">‚Äã</a></h4>
<p>Label smoothing value epsilon. Probability of all non-true labels will be smoothed by epsilon/(vocab_size-1). Set to 0 to turn off label smoothing. (<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1512.00567</a>)</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-lm_prior_lambda--float--00"><em>field</em> lm_prior_lambda <em>: float</em> <em>= 0.0</em><a href="#field-lm_prior_lambda--float--00" class="hash-link" aria-label="Direct link to field-lm_prior_lambda--float--00" title="Direct link to field-lm_prior_lambda--float--00">‚Äã</a></h4>
<p>LM Prior Lambda</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-lm_prior_model--str--none--none"><em>field</em> lm_prior_model <em>: str | None</em> <em>= None</em><a href="#field-lm_prior_model--str--none--none" class="hash-link" aria-label="Direct link to field-lm_prior_model--str--none--none" title="Direct link to field-lm_prior_model--str--none--none">‚Äã</a></h4>
<p>LM model to use to train the TM.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-lm_prior_tau--float--10"><em>field</em> lm_prior_tau <em>: float</em> <em>= 1.0</em><a href="#field-lm_prior_tau--float--10" class="hash-link" aria-label="Direct link to field-lm_prior_tau--float--10" title="Direct link to field-lm_prior_tau--float--10">‚Äã</a></h4>
<p>LM Prior Tau</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-loss_scale--float--00"><em>field</em> loss_scale <em>: float</em> <em>= 0.0</em><a href="#field-loss_scale--float--00" class="hash-link" aria-label="Direct link to field-loss_scale--float--00" title="Direct link to field-loss_scale--float--00">‚Äã</a></h4>
<p>For FP16 training, the static loss scale to use. If not set, the loss scale is dynamically computed.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-max_grad_norm--float--5"><em>field</em> max_grad_norm <em>: float</em> <em>= 5</em><a href="#field-max_grad_norm--float--5" class="hash-link" aria-label="Direct link to field-max_grad_norm--float--5" title="Direct link to field-max_grad_norm--float--5">‚Äã</a></h4>
<p>If the norm of the gradient vector exceeds this value, renormalize it to have the norm equal to max_grad_norm.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-normalization--literalsents-tokens--sents"><em>field</em> normalization <em>: Literal[&#x27;sents&#x27;, &#x27;tokens&#x27;]</em> <em>= &#x27;sents&#x27;</em><a href="#field-normalization--literalsents-tokens--sents" class="hash-link" aria-label="Direct link to field-normalization--literalsents-tokens--sents" title="Direct link to field-normalization--literalsents-tokens--sents">‚Äã</a></h4>
<p>Normalization method of the gradient.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-num_workers--int--2"><em>field</em> num_workers <em>: int</em> <em>= 2</em><a href="#field-num_workers--int--2" class="hash-link" aria-label="Direct link to field-num_workers--int--2" title="Direct link to field-num_workers--int--2">‚Äã</a></h4>
<p>Number of workers for pytorch.DataLoader objects.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-param_init--float--01"><em>field</em> param_init <em>: float</em> <em>= 0.1</em><a href="#field-param_init--float--01" class="hash-link" aria-label="Direct link to field-param_init--float--01" title="Direct link to field-param_init--float--01">‚Äã</a></h4>
<p>Support value for uniform distribution parameters initialization. Set to 0 not to use initialization.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-param_init_glorot--bool--false"><em>field</em> param_init_glorot <em>: bool</em> <em>= False</em><a href="#field-param_init_glorot--bool--false" class="hash-link" aria-label="Direct link to field-param_init_glorot--bool--false" title="Direct link to field-param_init_glorot--bool--false">‚Äã</a></h4>
<p>Initialize parameters with xavier_uniform. Required for transformer.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-pre_word_vecs_dec--str--none--none"><em>field</em> pre_word_vecs_dec <em>: str | None</em> <em>= None</em><a href="#field-pre_word_vecs_dec--str--none--none" class="hash-link" aria-label="Direct link to field-pre_word_vecs_dec--str--none--none" title="Direct link to field-pre_word_vecs_dec--str--none--none">‚Äã</a></h4>
<p>If a valid path is specified, will load pretrained word embeddings on the decoder side.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-pre_word_vecs_enc--str--none--none"><em>field</em> pre_word_vecs_enc <em>: str | None</em> <em>= None</em><a href="#field-pre_word_vecs_enc--str--none--none" class="hash-link" aria-label="Direct link to field-pre_word_vecs_enc--str--none--none" title="Direct link to field-pre_word_vecs_enc--str--none--none">‚Äã</a></h4>
<p>If a valid path is specified, will load pretrained word embeddings on the encoder side.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-prefetch_factor--int--200"><em>field</em> prefetch_factor <em>: int</em> <em>= 200</em><a href="#field-prefetch_factor--int--200" class="hash-link" aria-label="Direct link to field-prefetch_factor--int--200" title="Direct link to field-prefetch_factor--int--200">‚Äã</a></h4>
<p>Number of mini-batches loaded in advance to avoid the GPU waiting during processing of next bucket.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-save_checkpoint_steps--int--5000"><em>field</em> save_checkpoint_steps <em>: int</em> <em>= 5000</em><a href="#field-save_checkpoint_steps--int--5000" class="hash-link" aria-label="Direct link to field-save_checkpoint_steps--int--5000" title="Direct link to field-save_checkpoint_steps--int--5000">‚Äã</a></h4>
<p>Frequency of checkpoint saving (in steps).</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-save_format--literalpytorch-safetensors--pytorch"><em>field</em> save_format <em>: Literal[&#x27;pytorch&#x27;, &#x27;safetensors&#x27;]</em> <em>= &#x27;pytorch&#x27;</em><a href="#field-save_format--literalpytorch-safetensors--pytorch" class="hash-link" aria-label="Direct link to field-save_format--literalpytorch-safetensors--pytorch" title="Direct link to field-save_format--literalpytorch-safetensors--pytorch">‚Äã</a></h4>
<p>Format to save the model weights.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-score_threshold--float--068"><em>field</em> score_threshold <em>: float</em> <em>= 0.68</em><a href="#field-score_threshold--float--068" class="hash-link" aria-label="Direct link to field-score_threshold--float--068" title="Direct link to field-score_threshold--float--068">‚Äã</a></h4>
<p>Threshold to filterout data</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-single_pass--bool--false"><em>field</em> single_pass <em>: bool</em> <em>= False</em><a href="#field-single_pass--bool--false" class="hash-link" aria-label="Direct link to field-single_pass--bool--false" title="Direct link to field-single_pass--bool--false">‚Äã</a></h4>
<p>Make a single pass over the training dataset.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-train_from--str--none--none"><em>field</em> train_from <em>: str | None</em> <em>= None</em><a href="#field-train_from--str--none--none" class="hash-link" aria-label="Direct link to field-train_from--str--none--none" title="Direct link to field-train_from--str--none--none">‚Äã</a></h4>
<p>Pretrained model/checkpoint weights to continue training from.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-train_steps--int--100000"><em>field</em> train_steps <em>: int</em> <em>= 100000</em><a href="#field-train_steps--int--100000" class="hash-link" aria-label="Direct link to field-train_steps--int--100000" title="Direct link to field-train_steps--int--100000">‚Äã</a></h4>
<p>Number of training steps.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-truncated_decoder--int--0"><em>field</em> truncated_decoder <em>: int</em> <em>= 0</em><a href="#field-truncated_decoder--int--0" class="hash-link" aria-label="Direct link to field-truncated_decoder--int--0" title="Direct link to field-truncated_decoder--int--0">‚Äã</a></h4>
<p>Truncated bptt.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-update_vocab--bool--false"><em>field</em> update_vocab <em>: bool</em> <em>= False</em><a href="#field-update_vocab--bool--false" class="hash-link" aria-label="Direct link to field-update_vocab--bool--false" title="Direct link to field-update_vocab--bool--false">‚Äã</a></h4>
<p>Update source and target existing vocabularies.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-use_ckpting--liststr--"><em>field</em> use_ckpting <em>: List[str]</em> <em>= []</em><a href="#field-use_ckpting--liststr--" class="hash-link" aria-label="Direct link to field-use_ckpting--liststr--" title="Direct link to field-use_ckpting--liststr--">‚Äã</a></h4>
<p>Use gradient checkpointing for those modules.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
<li><a href="#eole.config.training.TrainingConfig.checkpointing_layers"><code>checkpointing_layers</code></a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-valid_batch_size--int--32"><em>field</em> valid_batch_size <em>: int</em> <em>= 32</em><a href="#field-valid_batch_size--int--32" class="hash-link" aria-label="Direct link to field-valid_batch_size--int--32" title="Direct link to field-valid_batch_size--int--32">‚Äã</a></h4>
<p>Maximum batch size for validation.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-valid_steps--int--10000"><em>field</em> valid_steps <em>: int</em> <em>= 10000</em><a href="#field-valid_steps--int--10000" class="hash-link" aria-label="Direct link to field-valid_steps--int--10000" title="Direct link to field-valid_steps--int--10000">‚Äã</a></h4>
<p>Frequency of validation, in steps.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="field-zero_out_prompt_loss--bool--false"><em>field</em> zero_out_prompt_loss <em>: bool</em> <em>= False</em><a href="#field-zero_out_prompt_loss--bool--false" class="hash-link" aria-label="Direct link to field-zero_out_prompt_loss--bool--false" title="Direct link to field-zero_out_prompt_loss--bool--false">‚Äã</a></h4>
<p>Set the prompt loss to zero. Mostly for LLM finetuning. Will be enabled only if the insert_mask_before_placeholder transform is applied.</p>
<ul>
<li><strong>Validated by:</strong>
<ul>
<li><code>_validate_running_config</code></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="validator-checkpointing_layers----use_ckptingsource"><em>validator</em> checkpointing_layers  <em>¬ª</em>  <a href="#eole.config.training.TrainingConfig.use_ckpting"><em>use_ckpting</em></a><a href="https://github.com/eole-nlp/eole/blob/master/eole/config/training.py#L295-L301" target="_blank" rel="noopener noreferrer">[source]</a><a href="#validator-checkpointing_layers----use_ckptingsource" class="hash-link" aria-label="Direct link to validator-checkpointing_layers----use_ckptingsource" title="Direct link to validator-checkpointing_layers----use_ckptingsource">‚Äã</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="get_model_pathsource">get_model_path()<a href="https://github.com/eole-nlp/eole/blob/master/eole/config/training.py#L303-L308" target="_blank" rel="noopener noreferrer">[source]</a><a href="#get_model_pathsource" class="hash-link" aria-label="Direct link to get_model_pathsource" title="Direct link to get_model_pathsource">‚Äã</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="property-storage_dtype--dtypesource"><em>property</em> storage_dtype <em>: dtype</em><a href="https://github.com/eole-nlp/eole/blob/master/eole/config/training.py#L273-L293" target="_blank" rel="noopener noreferrer">[source]</a><a href="#property-storage_dtype--dtypesource" class="hash-link" aria-label="Direct link to property-storage_dtype--dtypesource" title="Direct link to property-storage_dtype--dtypesource">‚Äã</a></h4>
<p>Deduce which dtype to use for main model parameters.
E.g. with mixed precision a copy is kept in float32.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/eole-nlp/eole/tree/main/docs/docs/reference/Config/training.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/eole/docs/reference/Config/run"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Main Entrypoints</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/eole/docs/reference/Core API/core"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Framework</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#pydantic-model-eoleconfigtrainingoptimizerconfigsource" class="table-of-contents__link toc-highlight"><em>pydantic model</em> eole.config.training.OptimizerConfig[source]</a></li><li><a href="#pydantic-model-eoleconfigtrainingtrainingconfigsource" class="table-of-contents__link toc-highlight"><em>pydantic model</em> eole.config.training.TrainingConfig[source]</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/eole/docs/">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eole-nlp/eole/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eole-nlp/eole" target="_blank" rel="noopener noreferrer" class="footer__link-item">Source<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">EOLE is an open-source toolkit and is licensed under the MIT license.</div></div></div></footer></div>
</body>
</html>