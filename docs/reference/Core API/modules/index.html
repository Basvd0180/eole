<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-reference/Core API/modules" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.2.0">
<title data-rh="true">Modules | Eole - üë∑‚Äç‚ôÇÔ∏èüöß Work In Progress</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://eole-nlp.github.io/eole/docs/reference/Core API/modules"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Modules | Eole - üë∑‚Äç‚ôÇÔ∏èüöß Work In Progress"><meta data-rh="true" name="description" content="Embeddings"><meta data-rh="true" property="og:description" content="Embeddings"><link data-rh="true" rel="icon" href="/eole/img/eole-logo.ico"><link data-rh="true" rel="canonical" href="https://eole-nlp.github.io/eole/docs/reference/Core API/modules"><link data-rh="true" rel="alternate" href="https://eole-nlp.github.io/eole/docs/reference/Core API/modules" hreflang="en"><link data-rh="true" rel="alternate" href="https://eole-nlp.github.io/eole/docs/reference/Core API/modules" hreflang="x-default"><link rel="stylesheet" href="/eole/assets/css/styles.0e100862.css">
<script src="/eole/assets/js/runtime~main.418f06a0.js" defer="defer"></script>
<script src="/eole/assets/js/main.782b1c90.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/eole/"><div class="navbar__logo"><img src="/eole/img/eole-logo.png" alt="Eole Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/eole/img/eole-logo.png" alt="Eole Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/eole/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/eole/docs/reference/index">Reference</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/eole-nlp/eole" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/reference/index">Eole Core API</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/eole/docs/reference/Config/">Configuration</a><button aria-label="Expand sidebar category &#x27;Configuration&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/eole/docs/reference/Core API/core">Core API</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Core API/core">Framework</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/eole/docs/reference/Core API/modules">Modules</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Core API/dataloaders">Data Loaders</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/reference/Core API/inference">Prediction</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/reference/bibliography">Bibliography</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/eole/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Core API</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Modules</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Modules</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="embeddings">Embeddings<a href="#embeddings" class="hash-link" aria-label="Direct link to Embeddings" title="Direct link to Embeddings">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesembeddingsword_vec_size-word_vocab_size-word_padding_idx-position_encodingfalse-position_encoding_typesinusoidalinterleaved-dropout0-sparsefalse-freeze_word_vecsfalsesource"><em>class</em> eole.modules.Embeddings(word_vec_size, word_vocab_size, word_padding_idx, position_encoding=False, position_encoding_type=&#x27;SinusoidalInterleaved&#x27;, dropout=0, sparse=False, freeze_word_vecs=False)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/embeddings.py#L79-L169" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesembeddingsword_vec_size-word_vocab_size-word_padding_idx-position_encodingfalse-position_encoding_typesinusoidalinterleaved-dropout0-sparsefalse-freeze_word_vecsfalsesource" class="hash-link" aria-label="Direct link to class-eolemodulesembeddingsword_vec_size-word_vocab_size-word_padding_idx-position_encodingfalse-position_encoding_typesinusoidalinterleaved-dropout0-sparsefalse-freeze_word_vecsfalsesource" title="Direct link to class-eolemodulesembeddingsword_vec_size-word_vocab_size-word_padding_idx-position_encodingfalse-position_encoding_typesinusoidalinterleaved-dropout0-sparsefalse-freeze_word_vecsfalsesource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Words embeddings for encoder/decoder.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>word_vec_size</strong> (<em>int</em>) ‚Äì size of the dictionary of embeddings.</li>
<li><strong>word_vocab_size</strong> (<em>int</em>) ‚Äì size of dictionary of embeddings for words.</li>
<li><strong>word_padding_idx</strong> (<em>int</em>) ‚Äì padding index for words in the embeddings.</li>
<li><strong>position_encoding</strong> (<em>bool</em>) ‚Äì see <a href="#eole.modules.PositionalEncoding"><code>PositionalEncoding</code></a></li>
<li><strong>dropout</strong> (<em>float</em>) ‚Äì dropout probability.</li>
<li><strong>sparse</strong> (<em>bool</em>) ‚Äì sparse embbedings default False</li>
<li><strong>freeze_word_vecs</strong> (<em>bool</em>) ‚Äì freeze weights of word vectors.</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardsource-stepnonesource">forward(source, step=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/embeddings.py#L150-L166" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardsource-stepnonesource" class="hash-link" aria-label="Direct link to forwardsource-stepnonesource" title="Direct link to forwardsource-stepnonesource">‚Äã</a></h4>
<p>Computes the embeddings for words.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>source</strong> (<em>LongTensor</em>) ‚Äì index tensor <code>(batch, len)</code></li>
<li><strong>Returns:</strong>
Word embeddings <code>(batch, len, embedding_size)</code></li>
<li><strong>Return type:</strong>
FloatTensor</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="load_pretrained_vectorsemb_filesource">load_pretrained_vectors(emb_file)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/embeddings.py#L133-L148" target="_blank" rel="noopener noreferrer">[source]</a><a href="#load_pretrained_vectorsemb_filesource" class="hash-link" aria-label="Direct link to load_pretrained_vectorsemb_filesource" title="Direct link to load_pretrained_vectorsemb_filesource">‚Äã</a></h4>
<p>Load in pretrained embeddings.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>emb_file</strong> (<em>str</em>) ‚Äì path to torch serialized embeddings</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulespositionalencodingdim-enc_type-max_len5000source"><em>class</em> eole.modules.PositionalEncoding(dim, enc_type, max_len=5000)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/embeddings.py#L14-L76" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulespositionalencodingdim-enc_type-max_len5000source" class="hash-link" aria-label="Direct link to class-eolemodulespositionalencodingdim-enc_type-max_len5000source" title="Direct link to class-eolemodulespositionalencodingdim-enc_type-max_len5000source">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Sinusoidal positional encoding for non-recurrent neural networks.</p>
<p>Implementation based on ‚ÄúAttention Is All You Need‚Äù
[]</p>
<ul>
<li><strong>Parameters:</strong>
<strong>dim</strong> (<em>int</em>) ‚Äì embedding size</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-stepnonesource">forward(emb, step=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/embeddings.py#L57-L76" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-stepnonesource" class="hash-link" aria-label="Direct link to forwardemb-stepnonesource" title="Direct link to forwardemb-stepnonesource">‚Äã</a></h4>
<p>Embed inputs.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>emb</strong> (<em>FloatTensor</em>) ‚Äì Sequence of word vectors
<code>(batch_size, seq_len, self.dim)</code></li>
<li><strong>step</strong> (<em>int</em> <em>or</em> <em>NoneType</em>) ‚Äì If stepwise (<code>seq_len = 1</code>), use
the encoding for this position.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesposition_ffnpositionwisefeedforwardmodel_config-running_confignonesource"><em>class</em> eole.modules.position_ffn.PositionwiseFeedForward(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/position_ffn.py#L28-L123" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesposition_ffnpositionwisefeedforwardmodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eolemodulesposition_ffnpositionwisefeedforwardmodel_config-running_confignonesource" title="Direct link to class-eolemodulesposition_ffnpositionwisefeedforwardmodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>A two-layer Feed-Forward-Network with residual layer norm.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> ‚Äì eole.config.models.ModelConfig object</li>
<li><strong>running_config</strong> ‚Äì TrainingConfig or InferenceConfig derived from RunningConfig</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardxsource">forward(x)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/position_ffn.py#L93-L119" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardxsource" class="hash-link" aria-label="Direct link to forwardxsource" title="Direct link to forwardxsource">‚Äã</a></h4>
<p>Layer definition.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>x</strong> ‚Äì <code>(batch_size, input_len, model_dim)</code></li>
<li><strong>Returns:</strong>
Output <code>(batch_size, input_len, model_dim)</code>.</li>
<li><strong>Return type:</strong>
(FloatTensor)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="encoders">Encoders<a href="#encoders" class="hash-link" aria-label="Direct link to Encoders" title="Direct link to Encoders">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoleencodersencoderbaseargs-kwargssource"><em>class</em> eole.encoders.EncoderBase(*args, **kwargs)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/encoder.py#L6-L37" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoleencodersencoderbaseargs-kwargssource" class="hash-link" aria-label="Direct link to class-eoleencodersencoderbaseargs-kwargssource" title="Direct link to class-eoleencodersencoderbaseargs-kwargssource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Base encoder class. Specifies the interface used by different encoder types
and required by :class:
eole.Models.EncoderDecoderModel
eole.Models.EncoderModel</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-masknonesource">forward(emb, mask=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/encoder.py#L19-L37" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-masknonesource" class="hash-link" aria-label="Direct link to forwardemb-masknonesource" title="Direct link to forwardemb-masknonesource">‚Äã</a></h4>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>emb</strong> (<em>FloatTensor</em>) ‚Äì embeddings <code>(batch, src_len, dim)</code></li>
<li><strong>mask</strong> (<em>BoolTensor</em>) ‚Äì mask <code>(batch, maxlen)</code> False when value, True when pad</li>
</ul>
</li>
<li><strong>Returns:</strong>
<ul>
<li>enc_out (encoder output used for attention),
<code>(batch, src_len, hidden_size)</code>
for bidirectional rnn last dimension is 2x hidden_size</li>
<li>enc_final_hs: encoder final hidden state
<code>(num_layers x dir, batch, hidden_size)</code>
In the case of LSTM this is a tuple.</li>
</ul>
</li>
<li><strong>Return type:</strong>
(FloatTensor, FloatTensor, FloatTensor)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoleencoderstransformerencodermodel_config-running_confignonesource"><em>class</em> eole.encoders.TransformerEncoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/transformer.py#L89-L160" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoleencoderstransformerencodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoleencoderstransformerencodermodel_config-running_confignonesource" title="Direct link to class-eoleencoderstransformerencodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.encoders.EncoderBase"><code>EncoderBase</code></a></p>
<p>The Transformer encoder from ‚ÄúAttention is All You Need‚Äù
[]</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> (<em>eole.config.TransformerEncoderConfig</em>) ‚Äì full encoder config</li>
<li><strong>embeddings</strong> (<a href="#eole.modules.Embeddings"><em>eole.modules.Embeddings</em></a>) ‚Äì embeddings to use, should have positional encodings</li>
<li><strong>running_config</strong> (<em>TrainingConfig / InferenceConfig</em>)</li>
</ul>
</li>
<li><strong>Returns:</strong>
<ul>
<li>enc_out <code>(batch_size, src_len, model_dim)</code></li>
<li>encoder final state: None in the case of Transformer</li>
<li>src_len <code>(batch_size)</code></li>
</ul>
</li>
<li><strong>Return type:</strong>
(torch.FloatTensor, torch.FloatTensor)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-masknonesource-1">forward(emb, mask=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/transformer.py#L143-L156" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-masknonesource-1" class="hash-link" aria-label="Direct link to forwardemb-masknonesource-1" title="Direct link to forwardemb-masknonesource-1">‚Äã</a></h4>
<p>See <a href="#eole.encoders.EncoderBase.forward"><code>EncoderBase.forward()</code></a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/transformer.py#L135-L141" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource" title="Direct link to classmethod-from_configmodel_config-running_confignonesource">‚Äã</a></h4>
<p>Alternate constructor.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoleencodersrnnencodermodel_config-running_confignonesource"><em>class</em> eole.encoders.RNNEncoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/rnn_encoder.py#L8-L97" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoleencodersrnnencodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoleencodersrnnencodermodel_config-running_confignonesource" title="Direct link to class-eoleencodersrnnencodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.encoders.EncoderBase"><code>EncoderBase</code></a></p>
<p>A generic recurrent neural network encoder.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> (<em>eole.config.ModelConfig</em>)</li>
<li><strong>running_config</strong> (<em>TrainingConfig / InferenceConfig</em>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-masknonesource-2">forward(emb, mask=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/rnn_encoder.py#L46-L54" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-masknonesource-2" class="hash-link" aria-label="Direct link to forwardemb-masknonesource-2" title="Direct link to forwardemb-masknonesource-2">‚Äã</a></h4>
<p>See <a href="#eole.encoders.EncoderBase.forward"><code>EncoderBase.forward()</code></a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource-1"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/rnn_encoder.py#L41-L44" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource-1" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource-1" title="Direct link to classmethod-from_configmodel_config-running_confignonesource-1">‚Äã</a></h4>
<p>Alternate constructor.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoleencoderscnnencodermodel_config-running_confignonesource"><em>class</em> eole.encoders.CNNEncoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/cnn_encoder.py#L12-L53" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoleencoderscnnencodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoleencoderscnnencodermodel_config-running_confignonesource" title="Direct link to class-eoleencoderscnnencodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.encoders.EncoderBase"><code>EncoderBase</code></a></p>
<p>Encoder based on ‚ÄúConvolutional Sequence to Sequence Learning‚Äù
[].</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-masknonesource-3">forward(emb, mask=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/cnn_encoder.py#L40-L50" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-masknonesource-3" class="hash-link" aria-label="Direct link to forwardemb-masknonesource-3" title="Direct link to forwardemb-masknonesource-3">‚Äã</a></h4>
<p>See <a href="#eole.encoders.EncoderBase.forward"><code>EncoderBase.forward()</code></a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource-2"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/cnn_encoder.py#L31-L38" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource-2" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource-2" title="Direct link to classmethod-from_configmodel_config-running_confignonesource-2">‚Äã</a></h4>
<p>Alternate constructor.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoleencodersmeanencodermodel_config-running_confignonesource"><em>class</em> eole.encoders.MeanEncoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/mean_encoder.py#L6-L41" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoleencodersmeanencodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoleencodersmeanencodermodel_config-running_confignonesource" title="Direct link to class-eoleencodersmeanencodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.encoders.EncoderBase"><code>EncoderBase</code></a></p>
<p>A trivial non-recurrent encoder. Simply applies mean pooling.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> (<em>eole.config.ModelConfig</em>)</li>
<li><strong>embeddings</strong> (<a href="#eole.modules.Embeddings"><em>eole.modules.Embeddings</em></a>) ‚Äì embeddings to use, should have positional encodings</li>
<li><strong>running_config</strong> (<em>TrainingConfig / InferenceConfig</em>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-masknonesource-4">forward(emb, mask=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/mean_encoder.py#L26-L41" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-masknonesource-4" class="hash-link" aria-label="Direct link to forwardemb-masknonesource-4" title="Direct link to forwardemb-masknonesource-4">‚Äã</a></h4>
<p>See <a href="#eole.encoders.EncoderBase.forward"><code>EncoderBase.forward()</code></a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource-3"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/encoders/mean_encoder.py#L20-L24" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource-3" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource-3" title="Direct link to classmethod-from_configmodel_config-running_confignonesource-3">‚Äã</a></h4>
<p>Alternate constructor.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="decoders">Decoders<a href="#decoders" class="hash-link" aria-label="Direct link to Decoders" title="Direct link to Decoders">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoledecodersdecoderbaseattentionaltruesource"><em>class</em> eole.decoders.DecoderBase(attentional=True)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/decoder.py#L4-L33" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoledecodersdecoderbaseattentionaltruesource" class="hash-link" aria-label="Direct link to class-eoledecodersdecoderbaseattentionaltruesource" title="Direct link to class-eoledecodersdecoderbaseattentionaltruesource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Abstract class for decoders.</p>
<ul>
<li><strong>Parameters:</strong>
<strong>attentional</strong> (<em>bool</em>) ‚Äì The decoder returns non-empty attention.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource-4"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/decoder.py#L17-L24" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource-4" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource-4" title="Direct link to classmethod-from_configmodel_config-running_confignonesource-4">‚Äã</a></h4>
<p>Alternate constructor.</p>
<p>Subclasses should override this method.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoledecoderstransformerdecodermodel_config-running_confignonesource"><em>class</em> eole.decoders.TransformerDecoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/transformer_decoder.py#L138-L258" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoledecoderstransformerdecodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoledecoderstransformerdecodermodel_config-running_confignonesource" title="Direct link to class-eoledecoderstransformerdecodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <code>TransformerDecoderBase</code></p>
<p>The Transformer decoder from ‚ÄúAttention is All You Need‚Äù.
[]</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> (<em>eole.config.TransformerDecoderConfig</em>) ‚Äì full decoder config</li>
<li><strong>embeddings</strong> (<a href="#eole.modules.Embeddings"><em>eole.modules.Embeddings</em></a>) ‚Äì embeddings to use, should have positional encodings</li>
<li><strong>running_config</strong> (<em>TrainingConfig / InferenceConfig</em>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-kwargssource">forward(emb, **kwargs)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/transformer_decoder.py#L168-L225" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-kwargssource" class="hash-link" aria-label="Direct link to forwardemb-kwargssource" title="Direct link to forwardemb-kwargssource">‚Äã</a></h4>
<p>Decode, possibly stepwise.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoledecodersrnn_decoderrnndecoderbasemodel_config-running_confignonesource"><em>class</em> eole.decoders.rnn_decoder.RNNDecoderBase(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/rnn_decoder.py#L9-L169" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoledecodersrnn_decoderrnndecoderbasemodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoledecodersrnn_decoderrnndecoderbasemodel_config-running_confignonesource" title="Direct link to class-eoledecodersrnn_decoderrnndecoderbasemodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.decoders.DecoderBase"><code>DecoderBase</code></a></p>
<p>Base recurrent attention-based decoder class.</p>
<p>Specifies the interface used by different decoder types
and required by <a href="/eole/docs/reference/Core API/core#eole.models.BaseModel"><code>BaseModel</code></a>.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> (<em>eole.config.DecoderConfig</em>) ‚Äì full decoder config</li>
<li><strong>running_config</strong> (<em>TrainingConfig / InferenceConfig</em>)</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-enc_out-src_lennone-stepnone-kwargssource">forward(emb, enc_out, src_len=None, step=None, **kwargs)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/rnn_decoder.py#L125-L166" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-enc_out-src_lennone-stepnone-kwargssource" class="hash-link" aria-label="Direct link to forwardemb-enc_out-src_lennone-stepnone-kwargssource" title="Direct link to forwardemb-enc_out-src_lennone-stepnone-kwargssource">‚Äã</a></h4>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>emb</strong> (<em>FloatTensor</em>) ‚Äì input embeddings
<code>(batch, tgt_len, dim)</code>.</li>
<li><strong>enc_out</strong> (<em>FloatTensor</em>) ‚Äì vectors from the encoder
<code>(batch, src_len, hidden)</code>.</li>
<li><strong>src_len</strong> (<em>LongTensor</em>) ‚Äì the padded source lengths
<code>(batch,)</code>.</li>
</ul>
</li>
<li><strong>Returns:</strong>
<ul>
<li>dec_outs: output from the decoder (after attn)
<code>(batch, tgt_len, hidden)</code>.</li>
<li>attns: distribution over src at each tgt
<code>(batch, tgt_len, src_len)</code>.</li>
</ul>
</li>
<li><strong>Return type:</strong>
(FloatTensor, dict[str, FloatTensor])</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource-5"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/rnn_decoder.py#L68-L75" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource-5" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource-5" title="Direct link to classmethod-from_configmodel_config-running_confignonesource-5">‚Äã</a></h4>
<p>Alternate constructor.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="init_statekwargssource">init_state(**kwargs)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/rnn_decoder.py#L77-L105" target="_blank" rel="noopener noreferrer">[source]</a><a href="#init_statekwargssource" class="hash-link" aria-label="Direct link to init_statekwargssource" title="Direct link to init_statekwargssource">‚Äã</a></h4>
<p>Initialize decoder state with last state of the encoder.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoledecodersstdrnndecodermodel_config-running_confignonesource"><em>class</em> eole.decoders.StdRNNDecoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/rnn_decoder.py#L172-L252" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoledecodersstdrnndecodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoledecodersstdrnndecodermodel_config-running_confignonesource" title="Direct link to class-eoledecodersstdrnndecodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.decoders.rnn_decoder.RNNDecoderBase"><code>RNNDecoderBase</code></a></p>
<p>Standard fully batched RNN decoder with attention.</p>
<p>Faster implementation, uses CuDNN for implementation.
See <code>RNNDecoderBase</code> for options.</p>
<p>Based around the approach from
‚ÄúNeural Machine Translation By Jointly Learning To Align and Translate‚Äù
[]</p>
<p>Implemented without input_feeding and currently with no coverage_attn</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoledecodersinputfeedrnndecodermodel_config-running_confignonesource"><em>class</em> eole.decoders.InputFeedRNNDecoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/rnn_decoder.py#L255-L334" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoledecodersinputfeedrnndecodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoledecodersinputfeedrnndecodermodel_config-running_confignonesource" title="Direct link to class-eoledecodersinputfeedrnndecodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.decoders.rnn_decoder.RNNDecoderBase"><code>RNNDecoderBase</code></a></p>
<p>Input feeding based decoder.</p>
<p>See <code>RNNDecoderBase</code> for options.</p>
<p>Based around the input feeding approach from
‚ÄúEffective Approaches to Attention-based Neural Machine Translation‚Äù
[]</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eoledecoderscnndecodermodel_config-running_confignonesource"><em>class</em> eole.decoders.CNNDecoder(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/cnn_decoder.py#L14-L125" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eoledecoderscnndecodermodel_config-running_confignonesource" class="hash-link" aria-label="Direct link to class-eoledecoderscnndecodermodel_config-running_confignonesource" title="Direct link to class-eoledecoderscnndecodermodel_config-running_confignonesource">‚Äã</a></h3>
<p>Bases: <a href="#eole.decoders.DecoderBase"><code>DecoderBase</code></a></p>
<p>Decoder based on ‚ÄúConvolutional Sequence to Sequence Learning‚Äù
[].</p>
<p>Consists of residual convolutional layers, with ConvMultiStepAttention.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardemb-enc_out-stepnone-kwargssource">forward(emb, enc_out, step=None, **kwargs)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/cnn_decoder.py#L76-L121" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardemb-enc_out-stepnone-kwargssource" class="hash-link" aria-label="Direct link to forwardemb-enc_out-stepnone-kwargssource" title="Direct link to forwardemb-enc_out-stepnone-kwargssource">‚Äã</a></h4>
<p>See <code>eole.modules.RNNDecoderBase.forward()</code></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="classmethod-from_configmodel_config-running_confignonesource-6"><em>classmethod</em> from_config(model_config, running_config=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/cnn_decoder.py#L53-L59" target="_blank" rel="noopener noreferrer">[source]</a><a href="#classmethod-from_configmodel_config-running_confignonesource-6" class="hash-link" aria-label="Direct link to classmethod-from_configmodel_config-running_confignonesource-6" title="Direct link to classmethod-from_configmodel_config-running_confignonesource-6">‚Äã</a></h4>
<p>Alternate constructor.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="init_statekwargssource-1">init_state(**kwargs)<a href="https://github.com/eole-nlp/eole/blob/master/eole/decoders/cnn_decoder.py#L61-L66" target="_blank" rel="noopener noreferrer">[source]</a><a href="#init_statekwargssource-1" class="hash-link" aria-label="Direct link to init_statekwargssource-1" title="Direct link to init_statekwargssource-1">‚Äã</a></h4>
<p>Init decoder state.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="attention">Attention<a href="#attention" class="hash-link" aria-label="Direct link to Attention" title="Direct link to Attention">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesglobalattentiondim-coveragefalse-attn_typedot-attn_funcsoftmaxsource"><em>class</em> eole.modules.GlobalAttention(dim, coverage=False, attn_type=&#x27;dot&#x27;, attn_func=&#x27;softmax&#x27;)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/global_attention.py#L15-L197" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesglobalattentiondim-coveragefalse-attn_typedot-attn_funcsoftmaxsource" class="hash-link" aria-label="Direct link to class-eolemodulesglobalattentiondim-coveragefalse-attn_typedot-attn_funcsoftmaxsource" title="Direct link to class-eolemodulesglobalattentiondim-coveragefalse-attn_typedot-attn_funcsoftmaxsource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Global attention takes a matrix and a query vector. It
then computes a parameterized convex combination of the matrix
based on the input query.</p>
<p>Constructs a unit mapping a query q of size dim
and a source matrix H of size n x dim, to an output
of size dim.</p>
<p>All models compute the output as
$c = \sum_{j=1}^{\text{SeqLength}} a_j H_j$ where
$a_j$ is the softmax of a score function.
Then then apply a projection layer to [q, c].</p>
<p>However they
differ on how they compute the attention score.</p>
<ul>
<li>
<p>Luong Attention (dot, general):
: * dot: $\text{score}(H_j,q) = H_j^T q$</p>
<ul>
<li>general: $\text{score}(H_j, q) = H_j^T W_a q$</li>
</ul>
</li>
<li>
<p>Bahdanau Attention (mlp):
: * $\text{score}(H_j, q) = v_a^T \text{tanh}(W_a q + U_a h_j)$</p>
</li>
<li>
<p><strong>Parameters:</strong></p>
<ul>
<li><strong>dim</strong> (<em>int</em>) ‚Äì dimensionality of query and key</li>
<li><strong>coverage</strong> (<em>bool</em>) ‚Äì use coverage term</li>
<li><strong>attn_type</strong> (<em>str</em>) ‚Äì type of attention to use, options [dot,general,mlp]</li>
<li><strong>attn_func</strong> (<em>str</em>) ‚Äì attention function to use, options [softmax,sparsemax]</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardsrc-enc_out-src_lennone-coveragenonesource">forward(src, enc_out, src_len=None, coverage=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/global_attention.py#L134-L197" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardsrc-enc_out-src_lennone-coveragenonesource" class="hash-link" aria-label="Direct link to forwardsrc-enc_out-src_lennone-coveragenonesource" title="Direct link to forwardsrc-enc_out-src_lennone-coveragenonesource">‚Äã</a></h4>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>src</strong> (<em>FloatTensor</em>) ‚Äì query vectors <code>(batch, tgt_len, dim)</code></li>
<li><strong>enc_out</strong> (<em>FloatTensor</em>) ‚Äì encoder out vectors <code>(batch, src_len, dim)</code></li>
<li><strong>src_len</strong> (<em>LongTensor</em>) ‚Äì source context lengths <code>(batch,)</code></li>
<li><strong>coverage</strong> (<em>FloatTensor</em>) ‚Äì None (not supported yet)</li>
</ul>
</li>
<li><strong>Returns:</strong>
<ul>
<li>Computed vector <code>(batch, tgt_len, dim)</code></li>
<li>Attention distribtutions for each query
<code>(batch, tgt_len, src_len)</code></li>
</ul>
</li>
<li><strong>Return type:</strong>
(FloatTensor, FloatTensor)</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="scoreh_t-h_ssource">score(h_t, h_s)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/global_attention.py#L100-L132" target="_blank" rel="noopener noreferrer">[source]</a><a href="#scoreh_t-h_ssource" class="hash-link" aria-label="Direct link to scoreh_t-h_ssource" title="Direct link to scoreh_t-h_ssource">‚Äã</a></h4>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>h_t</strong> (<em>FloatTensor</em>) ‚Äì sequence of queries <code>(batch, tgt_len, dim)</code></li>
<li><strong>h_s</strong> (<em>FloatTensor</em>) ‚Äì sequence of sources <code>(batch, src_len, dim</code></li>
</ul>
</li>
<li><strong>Returns:</strong>
raw attention scores (unnormalized) for each src index
: <code>(batch, tgt_len, src_len)</code></li>
<li><strong>Return type:</strong>
FloatTensor</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesmultiheadedattentionmodel_config-running_confignone-is_decoder-bool--true-attn_type-str--none--nonesource"><em>class</em> eole.modules.MultiHeadedAttention(model_config, running_config=None, is_decoder: bool = True, attn_type: str | None = None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/multi_headed_attn.py#L226-L737" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesmultiheadedattentionmodel_config-running_confignone-is_decoder-bool--true-attn_type-str--none--nonesource" class="hash-link" aria-label="Direct link to class-eolemodulesmultiheadedattentionmodel_config-running_confignone-is_decoder-bool--true-attn_type-str--none--nonesource" title="Direct link to class-eolemodulesmultiheadedattentionmodel_config-running_confignone-is_decoder-bool--true-attn_type-str--none--nonesource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Multi-Head Attention module from ‚ÄúAttention is All You Need‚Äù
[].</p>
<p>Similar to standard dot attention but uses
multiple attention distributions simulataneously
to select relevant items.</p>
<p>Also includes several additional tricks.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_config</strong> ‚Äì model_config: eole.config.models.ModelConfig object</li>
<li><strong>running_config</strong> ‚Äì TrainingConfig or InferenceConfig derived from RunningConfig</li>
<li><strong>is_decoder</strong> ‚Äì bool, true if called by the Decoder layers</li>
<li><strong>attn_type</strong> ‚Äì ‚Äúself‚Äù or ‚Äúcontext‚Äù</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardkey-tensor-value-tensor-query-tensor-mask-tensor--none--none-sliding_window-int--none--0-step-int--none--0-return_attn-bool--none--falsesource">forward(key: Tensor, value: Tensor, query: Tensor, mask: Tensor | None = None, sliding_window: int | None = 0, step: int | None = 0, return_attn: bool | None = False)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/multi_headed_attn.py#L407-L737" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardkey-tensor-value-tensor-query-tensor-mask-tensor--none--none-sliding_window-int--none--0-step-int--none--0-return_attn-bool--none--falsesource" class="hash-link" aria-label="Direct link to forwardkey-tensor-value-tensor-query-tensor-mask-tensor--none--none-sliding_window-int--none--0-step-int--none--0-return_attn-bool--none--falsesource" title="Direct link to forwardkey-tensor-value-tensor-query-tensor-mask-tensor--none--none-sliding_window-int--none--0-step-int--none--0-return_attn-bool--none--falsesource">‚Äã</a></h4>
<p>Compute the context vector and the attention vectors.</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>key</strong> (<em>Tensor</em>) ‚Äì set of key_len
key vectors <code>(batch, key_len, dim)</code></li>
<li><strong>value</strong> (<em>Tensor</em>) ‚Äì set of key_len
value vectors <code>(batch, key_len, dim)</code></li>
<li><strong>query</strong> (<em>Tensor</em>) ‚Äì set of query_len
query vectors  <code>(batch, query_len, dim)</code></li>
<li><strong>mask</strong> ‚Äì binary mask 1/0 indicating which keys have
zero / non-zero attention <code>(batch, query_len, key_len)</code></li>
<li><strong>step</strong> (<em>int</em>) ‚Äì decoding step (used for Rotary embedding)</li>
</ul>
</li>
<li><strong>Returns:</strong>
<ul>
<li>output context vectors <code>(batch, query_len, dim)</code></li>
<li>Attention vector in heads <code>(batch, head, query_len, key_len)</code>.</li>
</ul>
</li>
<li><strong>Return type:</strong>
(Tensor, Tensor)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesaverageattentionmodel_dim-dropout01-aan_useffnfalse-pos_ffn_activation_fnactivationfunctionrelusource"><em>class</em> eole.modules.AverageAttention(model_dim, dropout=0.1, aan_useffn=False, pos_ffn_activation_fn=ActivationFunction.relu)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/average_attn.py#L63-L126" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesaverageattentionmodel_dim-dropout01-aan_useffnfalse-pos_ffn_activation_fnactivationfunctionrelusource" class="hash-link" aria-label="Direct link to class-eolemodulesaverageattentionmodel_dim-dropout01-aan_useffnfalse-pos_ffn_activation_fnactivationfunctionrelusource" title="Direct link to class-eolemodulesaverageattentionmodel_dim-dropout01-aan_useffnfalse-pos_ffn_activation_fnactivationfunctionrelusource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Average Attention module from
‚ÄúAccelerating Neural Transformer via an Average Attention Network‚Äù
[].</p>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>model_dim</strong> (<em>int</em>) ‚Äì the dimension of keys/values/queries,
must be divisible by head_count</li>
<li><strong>dropout</strong> (<em>float</em>) ‚Äì dropout parameter</li>
<li><strong>pos_ffn_activation_fn</strong> (<em>ActivationFunction</em>) ‚Äì activation function choice for PositionwiseFeedForward layer</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardlayer_in-masknone-stepnonesource">forward(layer_in, mask=None, step=None)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/average_attn.py#L96-L126" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardlayer_in-masknone-stepnonesource" class="hash-link" aria-label="Direct link to forwardlayer_in-masknone-stepnonesource" title="Direct link to forwardlayer_in-masknone-stepnonesource">‚Äã</a></h4>
<ul>
<li><strong>Parameters:</strong>
<strong>layer_in</strong> (<em>FloatTensor</em>) ‚Äì <code>(batch, t_len, dim)</code></li>
<li><strong>Returns:</strong>
<ul>
<li>gating_out <code>(batch, tlen, dim)</code></li>
<li>average_out average attention
: <code>(batch, input_len, dim)</code></li>
</ul>
</li>
<li><strong>Return type:</strong>
(FloatTensor, FloatTensor)</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesconvmultistepattentioninput_sizesource"><em>class</em> eole.modules.ConvMultiStepAttention(input_size)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/conv_multi_step_attention.py#L17-L64" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesconvmultistepattentioninput_sizesource" class="hash-link" aria-label="Direct link to class-eolemodulesconvmultistepattentioninput_sizesource" title="Direct link to class-eolemodulesconvmultistepattentioninput_sizesource">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Conv attention takes a key matrix, a value matrix and a query vector.
Attention weight is calculated by key matrix with the query vector
and sum on the value matrix. And the same operation is applied
in each decode conv layer.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="apply_maskmasksource">apply_mask(mask)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/conv_multi_step_attention.py#L30-L32" target="_blank" rel="noopener noreferrer">[source]</a><a href="#apply_maskmasksource" class="hash-link" aria-label="Direct link to apply_maskmasksource" title="Direct link to apply_maskmasksource">‚Äã</a></h4>
<p>Apply mask</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardbase_target_emb-input_from_dec-encoder_out_top-encoder_out_combinesource">forward(base_target_emb, input_from_dec, encoder_out_top, encoder_out_combine)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/conv_multi_step_attention.py#L34-L64" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardbase_target_emb-input_from_dec-encoder_out_top-encoder_out_combinesource" class="hash-link" aria-label="Direct link to forwardbase_target_emb-input_from_dec-encoder_out_top-encoder_out_combinesource" title="Direct link to forwardbase_target_emb-input_from_dec-encoder_out_top-encoder_out_combinesource">‚Äã</a></h4>
<ul>
<li><strong>Parameters:</strong>
<ul>
<li><strong>base_target_emb</strong> ‚Äì target emb tensor
<code>(batch, channel, height, width)</code></li>
<li><strong>input_from_dec</strong> ‚Äì output of dec conv
<code>(batch, channel, height, width)</code></li>
<li><strong>encoder_out_top</strong> ‚Äì the key matrix for calc of attention weight,
which is the top output of encode conv</li>
<li><strong>encoder_out_combine</strong> ‚Äì the value matrix for the attention-weighted sum,
which is the combination of base emb and top output of encode</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="class-eolemodulesstructured_attentionmatrixtreeeps1e-05source"><em>class</em> eole.modules.structured_attention.MatrixTree(eps=1e-05)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/structured_attention.py#L6-L39" target="_blank" rel="noopener noreferrer">[source]</a><a href="#class-eolemodulesstructured_attentionmatrixtreeeps1e-05source" class="hash-link" aria-label="Direct link to class-eolemodulesstructured_attentionmatrixtreeeps1e-05source" title="Direct link to class-eolemodulesstructured_attentionmatrixtreeeps1e-05source">‚Äã</a></h3>
<p>Bases: <code>Module</code></p>
<p>Implementation of the matrix-tree theorem for computing marginals
of non-projective dependency parsing. This attention layer is used
in the paper ‚ÄúLearning Structured Text Representations‚Äù
[].</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="forwardinputsource">forward(input)<a href="https://github.com/eole-nlp/eole/blob/master/eole/modules/structured_attention.py#L17-L39" target="_blank" rel="noopener noreferrer">[source]</a><a href="#forwardinputsource" class="hash-link" aria-label="Direct link to forwardinputsource" title="Direct link to forwardinputsource">‚Äã</a></h4>
<p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="note">NOTE<a href="#note" class="hash-link" aria-label="Direct link to NOTE" title="Direct link to NOTE">‚Äã</a></h4>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/eole-nlp/eole/tree/main/docs/docs/reference/Core API/0_modules.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/eole/docs/reference/Core API/core"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Framework</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/eole/docs/reference/Core API/dataloaders"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Data Loaders</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#embeddings" class="table-of-contents__link toc-highlight">Embeddings</a><ul><li><a href="#class-eolemodulesembeddingsword_vec_size-word_vocab_size-word_padding_idx-position_encodingfalse-position_encoding_typesinusoidalinterleaved-dropout0-sparsefalse-freeze_word_vecsfalsesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.Embeddings(word_vec_size, word_vocab_size, word_padding_idx, position_encoding=False, position_encoding_type=&#39;SinusoidalInterleaved&#39;, dropout=0, sparse=False, freeze_word_vecs=False)[source]</a></li><li><a href="#class-eolemodulespositionalencodingdim-enc_type-max_len5000source" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.PositionalEncoding(dim, enc_type, max_len=5000)[source]</a></li><li><a href="#class-eolemodulesposition_ffnpositionwisefeedforwardmodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.position_ffn.PositionwiseFeedForward(model_config, running_config=None)[source]</a></li></ul></li><li><a href="#encoders" class="table-of-contents__link toc-highlight">Encoders</a><ul><li><a href="#class-eoleencodersencoderbaseargs-kwargssource" class="table-of-contents__link toc-highlight"><em>class</em> eole.encoders.EncoderBase(*args, **kwargs)[source]</a></li><li><a href="#class-eoleencoderstransformerencodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.encoders.TransformerEncoder(model_config, running_config=None)[source]</a></li><li><a href="#class-eoleencodersrnnencodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.encoders.RNNEncoder(model_config, running_config=None)[source]</a></li><li><a href="#class-eoleencoderscnnencodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.encoders.CNNEncoder(model_config, running_config=None)[source]</a></li><li><a href="#class-eoleencodersmeanencodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.encoders.MeanEncoder(model_config, running_config=None)[source]</a></li></ul></li><li><a href="#decoders" class="table-of-contents__link toc-highlight">Decoders</a><ul><li><a href="#class-eoledecodersdecoderbaseattentionaltruesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.decoders.DecoderBase(attentional=True)[source]</a></li><li><a href="#class-eoledecoderstransformerdecodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.decoders.TransformerDecoder(model_config, running_config=None)[source]</a></li><li><a href="#class-eoledecodersrnn_decoderrnndecoderbasemodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.decoders.rnn_decoder.RNNDecoderBase(model_config, running_config=None)[source]</a></li><li><a href="#class-eoledecodersstdrnndecodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.decoders.StdRNNDecoder(model_config, running_config=None)[source]</a></li><li><a href="#class-eoledecodersinputfeedrnndecodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.decoders.InputFeedRNNDecoder(model_config, running_config=None)[source]</a></li><li><a href="#class-eoledecoderscnndecodermodel_config-running_confignonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.decoders.CNNDecoder(model_config, running_config=None)[source]</a></li></ul></li><li><a href="#attention" class="table-of-contents__link toc-highlight">Attention</a><ul><li><a href="#class-eolemodulesglobalattentiondim-coveragefalse-attn_typedot-attn_funcsoftmaxsource" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.GlobalAttention(dim, coverage=False, attn_type=&#39;dot&#39;, attn_func=&#39;softmax&#39;)[source]</a></li><li><a href="#class-eolemodulesmultiheadedattentionmodel_config-running_confignone-is_decoder-bool--true-attn_type-str--none--nonesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.MultiHeadedAttention(model_config, running_config=None, is_decoder: bool = True, attn_type: str | None = None)[source]</a></li><li><a href="#class-eolemodulesaverageattentionmodel_dim-dropout01-aan_useffnfalse-pos_ffn_activation_fnactivationfunctionrelusource" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.AverageAttention(model_dim, dropout=0.1, aan_useffn=False, pos_ffn_activation_fn=ActivationFunction.relu)[source]</a></li><li><a href="#class-eolemodulesconvmultistepattentioninput_sizesource" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.ConvMultiStepAttention(input_size)[source]</a></li><li><a href="#class-eolemodulesstructured_attentionmatrixtreeeps1e-05source" class="table-of-contents__link toc-highlight"><em>class</em> eole.modules.structured_attention.MatrixTree(eps=1e-05)[source]</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/eole/docs/">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eole-nlp/eole/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eole-nlp/eole" target="_blank" rel="noopener noreferrer" class="footer__link-item">Source<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">EOLE is an open-source toolkit and is licensed under the MIT license.</div></div></div></footer></div>
</body>
</html>